{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52738e63",
   "metadata": {},
   "source": [
    "# Evaluation of Vanilla RAG Chain without Context Enrichment through auxillary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd36de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash-lite', google_api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6559a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('sample.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1afbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprint. Under review.\\nSpeculative Thinking: Enhancing Small-Model Reasoning\\nwith Large Model Guidance at Inference Time\\nWang Yang1, Xiang Yue2, Vipin Chaudhary1, Xiaotian Han1\\n1Case Western Reserve University 2Carnegie Mellon University\\n{wxy320,vxc204,xhan}@case.edu xyue2@andrew.cmu.edu\\nAbstract\\nRecent advances leverage post-training to enhance model reasoning perfor-\\nmance, which typically requires costly training pipelines and still suffers\\nfrom inefficient, overly lengthy outputs. We introduce Speculative Think-\\ning1, a training-free framework that enables large reasoning models to\\nguide smaller ones during inference at the reasoning level, distinct from\\nspeculative decoding, which operates at the token level. Our approach\\nis based on two observations: (1) reasoning-supportive tokens such as\\n“wait” frequently appear after structural delimiters like “\\\\n\\\\n”, serving as\\nsignals for reflection or continuation; and (2) larger models exhibit stronger\\ncontrol over reflective behavior, reducing unnecessary backtracking while\\nimproving reasoning quality. By strategically delegating reflective steps\\nto a more capable model, our method significantly boosts the reasoning\\naccuracy of reasoning models while shortening their output. With the assis-\\ntance of the 32B reasoning model, the 1.5B model’s accuracy on MATH500\\nincreases from 83.2% to 89.4%, marking a substantial improvement of 6.2%.\\nSimultaneously, the average output length is reduced from5439 tokens to\\n4583 tokens, representing a 15.7% decrease. Moreover, when applied to a\\nnon-reasoning model (Qwen-2.5-7B-Instruct), our framework boosts its\\naccuracy from 74.0% to 81.8% on the same benchmark, achieving a relative\\nimprovement of 7.8%.\\n+6.7%\\n-11.8%\\n(a) AIME\\n+6.2% -15.7% (b) MATH500\\n+8.1%\\n-4.0%\\n(c) GPQA\\n+5.0% -16.9% (d) AMC23\\nFigure 1: Speculative Thinking significantly improves the 1.5B model’s reasoning accuracy\\nwhile simultaneously reducing its average output length. This figure compares the accuracy\\nand average output length of models on four mathematical and reasoning datasets, includ-\\ning AIME 2020–2024, MATH500, GPQA, and AMC23. \"1.5B\" denotes the Deepseek-Distilled\\nQwen 2.5-1.5B model, \"32B\" refers to the Deepseek-Distilled Qwen 2.5-32B model, and\\n\"1.5B+32B\" represents our proposed Speculative Thinking method, where the 32B model\\nsupervises reflective reasoning steps of the 1.5B model during inference.\\n1Our code is available at https://github.com/uservan/speculative_thinking\\n1\\narXiv:2504.12329v1  [cs.CL]  12 Apr 2025Preprint. Under review.\\n1 Introduction\\nSmaller language models are widely used in real-world applications due to their lower com-\\nputational and memory requirements (Nguyen et al., 2024; Lu et al., 2025; Sui et al., 2025b).\\nHowever, they often underperform on tasks requiring complex reasoning (Li et al., 2025b;\\nSrivastava et al., 2025; Liu et al., 2025a). Improving their capabilities involves extensive\\npost-training such as supervised fine-tuning on high-quality reasoning traces (Chenglin\\net al., 2024) or reinforcement learning with verifiable signals (Shao et al., 2024; Chen et al.,\\n2025a; Zhang et al., 2024), which can be costly, data-intensive, and difficult to scale.\\nTo avoid retraining, inference-time scaling methods have been proposed to elicit better\\nintermediate steps from small models (Sui et al., 2025c; Xu et al., 2025). While lightweight\\nand training-free, these approaches depend entirely on the model’s existing abilities and\\noften yield limited or inconsistent improvements, particularly on complex tasks Li et al.\\n(2025b). Larger models, by contrast, exhibit significantly stronger reasoning abilities across\\na wide range of benchmarks (Muennighoff et al., 2025; Ye et al., 2025; Plaat et al., 2024), but\\ntheir inference cost and latency make them impractical for many deployment scenarios. This\\ntension motivates a central question: Can we improve small reasoning models during inference\\nby selectively leveraging large models, without additional training?\\nInspired by speculative decoding (Leviathan et al., 2023), which accelerates generation\\nby using a small model to propose tokens later verified by a larger model, we propose\\nSpeculative Thinking, a training-free framework for improving small-model reasoning\\nduring inference. Unlike speculative decoding, which operates at the token level, our\\napproach focuses on reasoning level. A small model generates most of the output but\\nselectively hands off difficult reasoning segments to a stronger model. These segments\\nare identified through structural cues—such as paragraph breaks (“\\\\n\\\\n”) followed by\\nreflective phrases like “wait” and “alternatively”—which often mark internal revision. Small\\nmodels frequently struggle in these cases, producing verbose outputs, while larger models\\nare more concise and effective at backtracking. By dynamically detecting these points and\\ndelegating them to a large mentor model, Speculative Thinking preserves the small model’s\\nefficiency while leveraging the large model’s strength exactly where it matters most.\\nEmpirical results demonstrate the effectiveness of this hybrid approach. A 1.5B model\\nassisted by Deepseek-distilled Qwen-2.5-32B improves by +6.6% on AIME , +6.2% on\\nMATH500 (Lightman et al., 2023), +8.1% onGPQA (Rein et al., 2024), and +5.0% onAMC23 ,\\nwhile reducing output length—indicating more efficient reasoning. Notably, this approach\\nis also effective for models not explicitly trained for reasoning: Qwen-2.5-7B-Instructgains\\n+7.8% on MATH500 and +14.2% on GPQA when assisted by the 32B mentor.\\nIn summary, Speculative Thinking offers a new inference-time paradigm that fuses the\\nefficiency of small models with the reasoning strength of large models. It opens a promising\\npath toward cost-effective reasoning augmentation for real-world inference.\\n2 Motivations\\n2.1 Analysis of LLM Reasoning Process\\nThis section investigates characteristic patterns that commonly emerge during the reasoning\\nprocesses of current reasoning models. By analyzing these patterns, we aim to uncover\\npotential avenues for enhancing and optimizing the models’ reasoning capabilities.\\n“\\\\n\\\\n” acts as a structural clue in model reasoning process. During inference, reasoning\\nmodels frequently generate certain reasoning-supportive tokens such as “wait”, “hmm” and\\n“alternatively”, which are relative with the model’s self-reflection behavior. To further ana-\\nlyze them, we examine the preceding token distribution for reasoning-supportive tokens in\\nDeepseek-distilled Qwen-2.5-32B on the MATH500 dataset. As shown in Table 1, we report\\nthe top 10 most frequent preceding tokens for three representative reasoning-supportive\\ntokens: “wait”, “alternatively”, and “hmm”. Notably, for all three tokens, the preceding\\ntoken is overwhelmingly dominated by the newline symbol “\\\\n\\\\n“. For instance, in the\\n2Preprint. Under review.\\nOkay, so I need to convert the rectangular coordinates.... \\\\n\\\\n First, let\\'s recall the formulas for converting \\nrectangular…\\\\n\\\\n Wait, but in this case, the x-coordinate is 0…. \\\\n\\\\n Now, for the angle θ, since x is 0 and y is \\npositive…. \\\\n\\\\n Let me verify that. If x is 0, then the point is …\\\\n\\\\n Is there a chance I made a mistake with \\nthe formula? Hmm, the formula ….\\\\n\\\\n Let me think of another way to remember it. ….\\\\n\\\\n I also know that in \\npolar coordinates….\\\\n\\\\n Wait a second, just to be thorough, let\\'s compute θ using the arctangent. ….\\\\n\\\\n Yeah, \\nso r is 3 and θ is π/2. So the polar coordinates should be (3, π/2).\\\\n\\\\n Wait, let me also make sure that …\\\\n\\\\nSo, \\nI think I\\'m confident that the polar coordinates for (0,3) are (3, π/2).\\nReflection AffirmationDeepseek-distilled Qwen-2.5-1.5B Statement\\nTo convert the point $(0, 3)$ from rectangular coordinates….\\\\n\\\\n ### Step 1: Calculate the Radius… \\\\n\\\\n For the point \\n$(0, 3)$:\\\\n\\\\\\\\[ r = \\\\\\\\sqrt{0^2 + 3^2} = \\\\\\\\sqrt{9} = 3 \\\\\\\\]...\\\\n\\\\n ### Step 2: Calculate the….\\\\n\\\\nFor the point $(0, 3)$:\\\\n- The \\nx-coordinate is $0$....\\\\n\\\\n This point lies on the positive y-axis. …\\\\n\\\\n ### Step 3: Final Answer\\\\n Combining the values \\nwe …\\\\n\\\\n Thus, the point $(0, 3)$ in rectangular coordinates can be represented as:\\\\n\\\\\\\\[\\\\n\\\\\\\\boxed{(3, \\\\\\\\frac{\\\\\\\\pi}{2})}\\\\n\\\\\\\\]\\nReflection AffirmationStatement\\n Qwen-2.5-1.5B-Instruct\\nFigure 2: Comparison of outputs between Reasoning Model and Non-reasoning model.\\nReasoning models often generate negative sentences—typically containing tokens such as\\n“wait”—immediately following the delimiter \"\\\\n\\\\n\". These sentences serve as reflective\\nprompts, helping the model to backtrack, reassess, and verify prior reasoning steps.\\nTable 1: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in\\nthe MATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-32B model. We\\nfind that over 80% of reasoning-supportive tokens appear after the occurrence of ”\\\\n\\\\n”,\\nindicating that it plays a crucial role in triggering reflective behavior during reasoning.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.928) \" \"(0.050) \").\\\\n\\\\n\"(0.007) \"?\\\\n\\\\n\"(0.006) \" \\\\n\\\\n\"(0.004)\\n\"].\\\\n\\\\n\"(0.002) \"\\\\n\\\\n\"(0.001) \")\\\\n\\\\n\"(0.001) \"]\\\\n\\\\n\"(0.001) \"?)\\\\n\\\\n\"(0.001)\\nhmm \" \"(0.690) \".\\\\n\\\\n\"(0.131) \"\\\\n\\\\n\"(0.044) \")\\\\n\\\\n\"(0.038) \").\\\\n\\\\n\"(0.035)\\n\"]\\\\n\\\\n\"(0.029) \" \\\\n\\\\n\"(0.009) \"?\\\\n\\\\n\"(0.007) \"?)\\\\n\\\\n\"(0.002) \"?\"\\\\n\\\\n\"(0.002)\\nwait \".\\\\n\\\\n\"(0.699) \" \"(0.182) \"?\\\\n\\\\n\"(0.039) \").\\\\n\\\\n\"(0.022) \"\\\\n\\\\n\"(0.017)\\n\")\\\\n\\\\n\"(0.011) \"]\\\\n\\\\n\"(0.007) \" \\\\n\\\\n\"(0.007) \":\\\\n\\\\n\"(0.004) \"].\\\\n\\\\n\"(0.002)\\ncase of “wait”, over 80% of its preceding tokens are “\\\\n\\\\n“. This strongly suggests that\\n“\\\\n\\\\n“ acts as a thinking cue—prompting the model to decide whether to reflect on the\\nprevious thought or proceed with the current line of reasoning. We have also extend this\\nsame analysis to other models on the MATH500 dataset in Appendix A.4.\\nCase analysis of LLM reasoning process to prove the role of \"\\\\n\\\\n\". To further prove the\\neffect of \"\\\\n\\\\n\", we conduct a case study on responses generated by Deepseek-distilled\\nQwen-2.5-1.5B and Qwen-2.5-1.5B-Instruct when answering questions in Figure 2. Specifi-\\ncally, we treat each occurrence of\"\\\\n\\\\n\"as a delimiter to segment the model’s output into\\nmultiple parts. We then categorize each segment as Affirmation, Reflection, or Statement:\\nAffirmation segments include affirming expressions such as yeah or yes, indicating a contin-\\nuation or endorsement of the preceding thought; Reflection segments contain expressions\\nlike wait, alternatively, or hmm, signaling the model’s intent to reflect its previous thought;\\nStatement segments often corresponding to formulaic expressions or factual outputs. Empir-\\nical analysis of representative examples in Figure 2 shows that the first sentence after each\\n”\\\\n\\\\n”often contains reasoning-related cues. This suggests that ”\\\\n\\\\n”acts as a discourse\\nmarker, prompting the model either affirm, reflect or state the previous thought.\\n2.2 Comparisons between Small and Large Reasoning Models\\nIn this section, we compare reasoning models of different sizes to find the differences\\nbetween small and large reasoning models, including Deepseek-distilled Qwen-2.5-32B, 7B,\\nand 1.5B. Specifically, we analyze their performance differences in terms of accuracy and\\noutput length on the AIME 2022-2024 dataset. All the results are shown in Figure 3 and the\\ndetailed statistics on other datasets can be found in Appendix A.5.\\n3Preprint. Under review.\\n1.5B 7B 32B\\n20\\n40\\n60Accuracy (%)25.56\\n48.89\\n65.56\\n1.5B 7B 32B\\n12000\\n14000\\n16000\\n18000Avg T oken Num\\n17798\\n13250\\n12274\\n1.5B 7B 32B\\n0\\n5000\\n10000\\n15000\\n20000\\n25000Average Number of T okens\\n#=23\\n#=67\\n#=44\\n#=46\\n#=59\\n#=31\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n2500\\n5000\\n7500\\n10000T otal Wait Count1078\\n10760\\n1838\\n5989\\n2377\\n4365\\nCorrect Wait Count Wrong Wait Count\\nFigure 3: Accuracy and output statistics of three models on the AIME 2022–2024 dataset.\\nReported metrics include: overall accuracy (upper left), average output length (upper right),\\naverage output length (down left) for correct and incorrect answers, as well as the number\\nof reflective sentences—such as those containing terms like “wait” or “alternatively”—in\\nboth correct and incorrect responses (down right). “#=67” indicates the number of incorrect\\nresponses made by the 1.5B model is 67. The average output length of small models is\\nsignificantly higher than that of large models. This is primarily due to the excessive length\\nof incorrect responses. At its core, this phenomenon stems from inefficient and redundant\\nself-reflection in small models, which often leads to failed reasoning attempts and ultimately\\nprevents them from arriving at correct answers before its max output length.\\nSmall reasoning models have worse reasoning performances and much longer responses.\\nWe first report the accuracy and average output length for all three models. As shown\\nin Figure 3, smaller models exhibit significantly lower accuracy compared to larger ones.\\nInterestingly, the average output length of smaller models tends to be much longer. As\\nmodel size increases, accuracy improves while outputs become more concise. To further\\nunderstand this phenomenon, we analyze the average lengths of correct and incorrect\\nresponses separately. We find that, across all model sizes, incorrect responses are consistently\\nmuch longer than correct ones. This suggests that the overall average output length is\\nheavily influenced by the proportion of incorrect answers, which are typically more verbose.\\nLarger-scale models exhibit more effective self-reflection and backtracking during reason-\\ning. To further investigate why incorrect responses are substantially longer than correct ones,\\nwe analyze the frequency of reflective phrases—such as“wait” and “alternatively”—which in-\\ndicate hesitation, self-reflection, or backtracking in reasoning process. As shown in Figure 3,\\nsuch phrases occur far more frequently in incorrect responses, particularly in smaller models.\\nThis suggests that smaller models tend to over-reflect yet under-reason, leading to inefficient\\nexploration of the solution space. Consequently, the excessive length of their outputs is\\nprimarily due to their inability to converge on correct answers within the maximum context\\nwindow, resulting in repetitive branching and redundant verification steps.\\n2.3 How to Combine Small and Large Reasoning Model?\\nWe observe that when reasoning models generate incorrect answers, their average output\\nlength increases significantly. A key manifestation of this is the overuse of words like “wait”,\\nindicating excessive self-reflection and backtracking. However, as model size increases,\\nsuch reflection becomes more efficient, resulting in fewer redundant revisions and shorter\\noutputs overall. This naturally raises an intriguing question: Can the reasoning ability of\\nlarger models be leveraged to monitor smaller models during inference?\\nWe propose a novel intervention strategy that utilizes the \"\\\\n\\\\n\" reasoning pattern as a\\ncontrol point for collaborative inference. In particular, when a smaller model encounters a\\n\"\\\\n\\\\n\"followed by tokens like ”wait”, which often signal confusion or indecision, we can\\ndelegate the subsequent reasoning step to a larger model because the larger one could give\\n4Preprint. Under review.\\nLarge Model (Target Model)\\nSmall Model (Speculative Model)\\nGuiding\\nThinking\\nFinal Answer is 706.   Convert the point...\\nOkay, so I need …  Wait, but maybe …\\n   So, yep, I think …Wait, seems it is…\\nSo, it is correct … Double-check it:\\nThe first step is …\\n   Convert the point...\\nFigure 4: Overview of speculative thinking. A small model generates most output but\\nselectively delegates challenging segments—marked by structural cues such as paragraph\\nbreaks (“\\\\n\\\\n”) followed by reflective phrases like “wait,” “alternatively,” or “hold on”—to\\na stronger model. Small models often produce verbose or incoherent outputs at these points,\\nwhile larger models handle them concisely. The proposed speculative thinking preserves\\nefficiency while leveraging the large model’s strength when most needed.\\na more accurate thinking step. The larger model then generates the next thought segment\\nin place of the smaller model, effectively acting as a reasoning supervisor or corrector. This\\nlarge-model-aided intervention may enhance the robustness and accuracy of smaller models\\nby injecting stronger reasoning capabilities, thus balancing efficiency and performance.\\n3 Method: Speculative Thinking\\nWe propose a collaborative inference framework termed Speculative Thinking, where a\\nsmall model acts as speculative model and a large model serves as target model. Speculative\\nmodel performs primary reasoning, while target model intervenes selectively to provide\\nauxiliary thoughts when necessary. The overall framework is in Figure 4, . Target model\\ntakes over speculative model’s generation under the following three scenarios. The hyper-\\nparameters for Speculative Thinking—such as the selection of Reflection and Affirmation\\nkeywords, and the values of control parameters n1, n2, and n3 are shown in Appendix A.2.\\n(1) Affirmation/Reflection Takeover. This mechanism leverages stronger reasoning ability\\nof target model to help speculative model decide whether to continue or revise. Speculative\\nmodel first generates responses until a delimiter token (e.g., \\\\n\\\\n) is encountered. After this\\ndelimiter, speculative model generates one full sentence (i.e., n1 tokens). We then classify\\nthe sentence into three situations: Affirmation, Reflection, or Statement, based on keyword\\nmatching, as shown in Appendix A.2. If speculative model’s sentence is classified as either\\nAffirmation or Reflection, target model immediately takes over and generates n1 tokens.\\nSpeculative model then resumes generation conditioned on target model’s output.\\n(2) Verification Takeover. We observe that small models often struggle with effective\\nverification. To address this, we introduce a verification-triggered intervention. Whenever a\\n\\\\n\\\\n delimiter is encountered—regardless of whether the subsequent sentence is generated\\nby the speculative or target model—we examine if the sentence contains verification-related\\ncues (e.g., verify, double-check, etc.). If such cues are detected, target model takes over to\\ngenerate n2 tokens, assisting the verification process and mitigating false conclusions.\\n(3) Excessive Reflection Takeover. Our analysis reveals that a hallmark of incorrect answers\\nis excessive backtracking, where the model repeatedly negates its own thoughts. To mitigate\\nthis, we implement a negativity counter c that tracks the number of reflection sentences.\\nEach time a \\\\n\\\\nis encountered, we evaluate whether the following sentence is negative; if\\nso, we increment c. Once c exceeds a predefined threshold, we prompt the model to exit the\\nreflection loop. Specifically, we insert an auxiliary sentence (e.g., “Let us check whether there\\nare some wrong steps.”) into the output, and then delegate the next n3 tokens to target model.\\nThis mechanism serves to reorient speculative model and prevent reflection thinking loops.\\n5Preprint. Under review.\\nTable 2: Accuracy, average output length, and estimated speed of models on four datasets.\\nHere, 1.5B refers to the Deepseek-Distilled Qwen-2.5-1.5B model. “+” means with the help\\nof large models. modify ratio indicates the proportion of tokens in the final output that\\ncome from the target model. After applying Speculative Thinking, both 1.5B and 7B models\\ndemonstrate improvements in accuracy, output length, and estimated inference speed. The\\nimprovement in estimated speed is measured relative to the corresponding target model.\\nDataset Speculative Target Modify Acc Length Estimated\\npass@1 Model Model Ratio (%) Improv. Avg Decr. Speed Improv.\\nAIME\\n1.5B\\n– – 25.6 – 17800.0 – 198.9 –\\n+14B 18.0% 33.3 +7.7 16691.2 -6.2% 110.3 +121.1%\\n+32B 19.0% 32.2 +6.6 15706.1 -11.7% 85.8 +185.9%\\n7B – – 48.9 – 13250.4 – 56.4 –\\n+32B 18.0% 53.3 +4.4 13213.6 -0.3% 41.0 +36.8%\\n14B – – 60.0 – 12600.2 – 49.9 –\\n32B – – 65.6 – 12274.3 – 30.0 –\\nGPQA\\n1.5B\\n– – 33.8 – 7922.0 – 223.2 –\\n+14B 15.0% 38.9 +5.1 8134.3 +2.7% 128.1 +121.7%\\n+32B 17.0% 41.9 +8.1 7612.4 -3.9% 91.8 +190.4%\\n7B – – 45.5 – 6111.5 – 62.1 –\\n+32B 22.0% 52.0 +6.5 5952.5 -2.6% 40.3 +27.5%\\n14B – – 57.1 – 5762.7 – 57.8 –\\n32B – – 61.6 – 5406.8 – 31.6 –\\nMATH500\\n1.5B\\n– – 83.2 – 5439.1 – 242.6 –\\n+14B 19.0% 89.0 +5.8 4527.4 -16.8% 134.6 +124.0%\\n+32B 19.0% 89.4 +6.2 4582.8 -15.7% 96.6 +200.0%\\n7B – – 92.8 – 3975.2 – 63.7 –\\n+32B 18.0% 93.0 +0.2 3767.8 -5.2% 46.0 +42.9%\\n14B – – 93.8 – 3609.0 – 60.1 –\\n32B – – 92.8 – 3802.2 – 32.2 –\\nAMC23\\n1.5B\\n– – 75.0 – 10460.8 – 212.7 –\\n+14B 19.0% 85.0 +10.0 7503.2 -28.3% 123.7 +123.0%\\n+32B 21.0% 80.0 +5.0 8691.2 -16.9% 82.8 +170.0%\\n7B – – 92.5 – 6093.8 – 62.6 –\\n+32B 16.0% 92.5 +0.0 5116.1 -16.1% 48.0 +56.4%\\n14B – – 95.0 – 6395.4 – 55.5 –\\n32B – – 95.0 – 7106.7 – 30.7 –\\n4 Experiments\\n4.1 Large Reasoning Models Monitor Small Reasoning Models\\nThis experiment aims to evaluate the effectiveness of Speculative Thinking. We adopt three\\nkey evaluation metrics: accuracy, average output length, and estimated inference speed,\\nto fully assess the trade-off between reasoning performance and efficiency. The rationale\\nfor choosing the estimated inference speed, along with the details of its computation, is\\nprovided at the end of this section. We conduct experiments on four benchmark datasets:\\nAIME 2022–2024, GPQA-Diamond, MATH500, and AMC23.\\nAnalysis of results of Large Reasoning Models Monitor Small Reasoning Models. The re-\\nsults are summarized in Table 2, which demonstrates that our method consistently improves\\naccuracy while reducing unnecessary output length and enhancing inference speed. For ex-\\nample, after being assisted by the 32B target model, the 1.5B speculative model demonstrates\\nconsistent and significant improvements across multiple datasets. Specifically, its accuracy\\nincreases by 6.2% on MATH500 , 8.1% on GPQA , 5.0% on AMC23 , and 6.6% on AIME . In\\naddition, the average output length is reduced by15.7%, 3.9%, 16.9% and 11.7% on the same\\ndatasets, respectively, indicating that the speculative model is able to reach conclusions\\nmore efficiently with guidance from the large model. Furthermore, in terms of estimated\\n6Preprint. Under review.\\ngeneration speed, the 1.5B model assisted by the 32B model consistently outperforms the\\nstandalone 32B model, despite leveraging it selectively. These findings collectively demon-\\nstrate the effectiveness and practicality of our Speculative Thinking framework, offering a\\npromising trade-off between performance and computational efficiency. Moreover, when\\nassisting the smaller reasoning model, the target model only needs to modify approximately\\n20% of the speculative model’s output to significantly enhance its reasoning performance.\\nFigure 5: A comparison between the prefix\\nand decode stages reveals that the time (in\\nseconds) required to process multiple tokens\\nduring the prefix phase is nearly equivalent\\nto the time taken to decode a single token.\\nModel decode prefix\\nn=1 n=1 n=20 n=250\\n1.5B 0.036 0.036 0.040 0.045\\n32B 0.09 0.11 0.12 0.15\\nTheoretical Estimation of FLOPs and To-\\nken Generation Speed. We adopt a theo-\\nretical analysis rather than empirical tim-\\ning, since our method— Speculative Think-\\ning—primarily introduces logical coordina-\\ntion between models. In contrast, runtime\\nmeasurements would be significantly af-\\nfected by backend GPU optimizations, es-\\npecially in systems like vLLM (Kwon et al.,\\n2023). The computation of FLOPs for prefill\\nand decode stages is in Appendix A.1. The\\ndifferences between prefix and decode are shown in Figure 5.\\nWe empirically profile average inference time for both decode and prefix stages across\\nvarious model sizes and output token lengths. These measurements are obtained using\\ngenerate() api from HuggingFace Transformers, with key-value cache enabled for the\\nprompt. We observe that when GPU memory are sufficient, the average time in prefix stage\\nremains relatively stable across positions. We could see time required to process multiple\\ntokens during the prefix phase is nearly equivalent to the time taken to decode a single\\ntoken. To reflect the difference, we assume a speedup for the prefix stage :FLOPsprefix(m) =\\nFLOPsdecode(n = 1), where m and n mean the token number. We set GPU computational\\ncapacity to 3.12 × 1010 FLOPs/s, which corresponds to a A100-class GPU. The estimated\\nspeed is calculated as follows:\\nEstimated Speed = Total Tokens\\x10\\nFLOPsprefill + FLOPsprefix + FLOPsdecode\\n\\x11\\n/GPU Capacity\\n(1)\\n4.2 Reasoning Models Monitor Non-Reasoning Models\\nGiven that large reasoning models can effectively assist smaller reasoning models, a natural\\nfollow-up question is: Can we leverage reasoning-capable models to enhance the performance\\nand accuracy of non-reasoning models ? To explore this, we adapt the Speculative Thinking\\nframework to monitor a speculative model that lacks inherent reasoning capability.\\nModification for speculative thinking applied to non-reasoning models. Specifically, in\\nAffirmation/Reflection Takeover, we originally determine whether the speculative model’s\\nsentence following a \"\\\\n\\\\n\" contains reflective or Affirmative reasoning cues. However,\\nnon-reasoning models typically do not emit such linguistic signals. Therefore, in this setting,\\nwe directly allow target model to take over and generate the next sentence after each\\n\"\\\\n\\\\n\". In addition, we further enhance the speculative model by allowing target model to\\ngenerate the first 100 tokens before any question answering begins. This is motivated by the\\nobservation that reasoning models often preface their answers with structured setups such\\nas “Okay, so I have this problem where I need...”, which helps guide the generation for models.\\nAnalysis of Results of Reasoning Models Monitor Non-Reasoning Models. The results,\\nwhere a non-reasoning model is augmented by a reasoning-capable target model, are shown\\nin Table 3. We first observe that Qwen-2.5-7B-Instruct, a non-reasoning model, benefits\\nnotably from speculative assistance by both 7B and 32B reasoning models. For instance,\\non the MATH500 dataset, its accuracy improves from 74.0% to 81.8%. However, this\\nimprovement comes at the cost of increased output length, indicating a trade-off between\\nenhanced reasoning ability and generation efficiency. However, when assisted by the 1.5B\\nreasoning model, performance improvements are not consistently observed. This indicates\\n7Preprint. Under review.\\nTable 3: Accuracy, average output length, and estimated speed on four datasets. 7B-Instruct\\nrefers to Qwen-2.5-7B-Instruct. “+” means with the help of reasoning models. Modify\\nratio indicates the proportion of tokens in the final output that come from target model.\\nAfter applying Speculative Thinking, models demonstrate improvements in accuracy. The\\nimprovement in estimated speed is measured relative to the corresponding target model.\\nDataset Speculative Target Avg Modify Estimated Acc\\npass@1 Model Model Length Ratio Speed (%) Improv.\\nAIME 7B-Instruct\\n– 1249.8 – 64.7 7.8 –\\n+1.5B 8029.3 54.0% 51.5 6.7 -1.1\\n+7B 10458.5 42.0% 38.8 13.3 +5.5\\n+32B 10236.0 46.0% 29.0 15.6 +7.8\\nGPQA 7B-Instruct\\n– 5.6 – 1.5 33.8 –\\n+1.5B 6763.8 43.0% 45.6 31.8 -2.0\\n+7B 4739.7 42.0% 36.8 40.9 +7.1\\n+32B 6652.8 31.0% 33.6 48.0 +14.2\\nMATH500 7B-Instruct\\n– 802.3 – 58.3 74.0 –\\n+1.5B 3368.8 43.0% 53.1 74.8 +0.8\\n+7B 3172.0 44.0% 41.2 79.2 +5.2\\n+32B 3015.9 44.0% 31.7 81.8 +7.8\\nAMC23 7B-Instruct\\n– 878.5 – 64.8 42.5 –\\n+1.5B 7603.0 49.0% 48.4 55.0 +12.5\\n+7B 6431.5 43.0% 39.0 67.5 +25.0\\n+32B 8732.8 31.0% 33.5 55.0 +12.5\\nthat, during the design of speculative thinking systems, it is preferable to choose a target\\nmodel that is either of equal size or larger than the speculative model, and more importantly,\\npossesses stronger reasoning capabilities. Mismatches where the speculative model is larger\\nor stronger than the target model may lead to suboptimal or even detrimental outcomes.\\n4.3 Comparisons between Speculative Decoding and Speculative Thinking\\nMATH500 AIME\\n60\\n80\\n100Accuracy (%)\\n92.8\\n65.6\\n93.0\\n58.9\\n93.0\\n53.3\\n32B\\nDecoding\\nThinking\\nMATH500 AIME\\n25\\n30\\n35\\n40\\n45\\n50Speed (token/s)\\n30.0\\n32.230.4 28.7\\n46.0\\n41.0\\nFigure 6: Comparison between Speculative Decoding and Thinking using a 7B speculative\\nmodel and a 32B target model. In Speculative Decoding, speculative model generates 20\\ntokens per step to match the number of intervention tokens in Speculative Thinking.\\nThis experiment primarily compares the differences between speculative decoding and\\nspeculative thinking. Due to the constraint that speculative decoding requires the specula-\\ntive model and the target model to have the same vocabulary size, we obtain speculative\\ndecoding results where the speculative model is 7B, and the target model is 32B. To align\\nwith Speculative Thinking, which takes over the generation of 20 tokens at a time, we set\\nthe speculative model in speculative decoding to generate n = 20 tokens per step.\\nSpeculative decoding relies on the speculative and target models having similar token\\noutput distributions to accelerate generation. In contrast, Speculative Thinking focuses on\\nenhancing the speculative model’s reasoning with lightweight assistance from target model,\\nwithout strictly requiring token distributional alignment. As shown in in Figure 6, although\\nspeculative decoding matchs the accuracy of 32B model, it often suffers from a high rejection\\n8Preprint. Under review.\\nrate—nearly 50% of tokens need to be regenerated by target model, which diminishes its\\nspeed. Speculative Thinking avoids this issue by allowing the target model to intervene\\nonly when necessary, improving the speculative model’s reasoning with minimal overhead.\\n5 Related Works\\nLLM Reasoning. Current approaches to enhancing the reasoning capabilities (Chen et al.,\\n2025a; Plaat et al., 2024; Sun et al., 2023) of language models primarily fall into two categories:\\nreinforcement learning (Schulman et al., 2017) and supervised fine-tuning (Jaech et al., 2024;\\nYang et al., 2024). For instance, DeepSeek (Guo et al., 2025; Liu et al., 2024) achieved\\nstate-of-the-art reasoning performance using GRPO (Shao et al., 2024; Yu et al., 2025), and\\nfurther improved smaller models by distilling high-quality reasoning traces. This line of\\nresearch has inspired numerous efforts to replicate DeepSeek-R1 with the goal of uncovering\\npotential “aha moments” in reasoning, including works such as Logic RL (Xie et al., 2025)\\nand SimpleRL-Zoo (Zeng et al., 2025). Many studies also use SFT to improve reasoning,\\nincluding SkyThought-T1 (Team, 2025b) and Bespoke-Stratos-32B (Labs, 2025), which collect\\nand fine-tune on carefully curated high-quality reasoning data. Several works have further\\ninvestigated key techniques for enhancing reasoning performance during RL (Baek &\\nTegmark, 2025; Yeo et al., 2025) or SFT (Chen et al., 2025b; 2024a; Tian et al., 2025; Liu et al.,\\n2025b). For example, (Li et al., 2025a) argues that the structure of reasoning steps in the\\ndata is more critical than the actual content; (Ji et al., 2025) highlights the importance of the\\ninitial few tokens in each reasoning instance for optimizing model performance. In addition,\\nseveral recent studies—such as s1(Muennighoff et al., 2025) emphasize the value of selecting\\na small set of high-quality reasoning samples to drive efficient model improvement.\\nEfficient Reasoning. Current reasoning models still exhibit notable limitations (Bandyopad-\\nhyay et al., 2025; Li et al., 2025c). One prominent issue is excessive response length—many\\nreasoning-enabled models tend to generate unnecessarily verbose outputs. As a result, effi-\\ncient reasoning has become an emerging research focus. An early effort in this direction was\\nproposed by Kimi 1.5 (Team et al., 2025), which introduced the Long-to-Short method. This\\napproach collects paired long and short responses and applies Direct Preference Optimiza-\\ntion (Rafailov et al., 2023; Zeng et al., 2024) to train models that prefer concise answers. The\\nidea was later reproduced by Sky-Thought (Team, 2025a), further validating its effectiveness.\\nTokenSkip (Xia et al., 2025), which improves efficiency by identifying and removing redun-\\ndant or uninformative tokens to create cleaner training data. LightThinker (Zhang et al.,\\n2025) takes a different route by explicitly compressing intermediate thoughts to generate\\nshorter yet informative reasoning traces, thereby enabling models to produce more concise\\noutputs via fine-tuning. Wang et al. (2025); Sui et al. (2025a) highlights a counterintuitive\\nphenomenon: when reasoning fails, model outputs often become significantly longer. This is\\nattributed to repetitive generation of reasoning-supportive tokens like “wait”, which reflect\\nthe model’s tendency to over-compensate by generating more thoughts. Other notable\\napproaches include Dynasor(Fu et al., 2024), which uses probing techniques to detect and\\nterminate reasoning early. There are some other works including efficient reaosning (Aytes\\net al., 2025; Lee et al., 2025; Sui et al., 2025c; Xu et al., 2025; Liao et al., 2025).\\n6 Conclusion\\nWe propose Speculative Thinking, a training-free framework that leverages larger reasoning\\nmodels to guide smaller ones through selective delegation at structurally meaningful points\\nin generation. By exploiting the natural reasoning patterns of LLMs—particularly reflec-\\ntion cues like \"\\\\n\\\\n\"—our approach significantly enhances both accuracy, average output\\nlength and efficiency without any additional training in four math reasoning datasets like\\nMATH500. Experiments demonstrate substantial gains in performance and output con-\\nciseness, underscoring the potential of collaborative inference between models of different\\ncapacities. This highlights a promising paradigm for improving reasoning of reasoning and\\nnon-reasoning models without additional data or training computation cost.\\n9Preprint. Under review.\\nLimitations\\nSpeculative Thinking relies on the assistance of a larger target model to improve the reason-\\ning ability and reduce the output length of a smaller speculative model. For this framework\\nto be effective, target model must possess stronger reasoning capabilities than speculative\\nmodel. Additionally, our current implementation assumes that both models belong to the\\nsame model family, which allows us to leverage shared KV cache structures to accelerate\\ninference. Finally, we observe that the performance of Speculative Thinking is sensitive\\nto prompt quality—utilizing an optimized prompt for each model is critical to achieving\\nthe best results, like “Please reason step by step, and put your final answer within\\n\\\\boxed{}.”.\\nReferences\\nSimon A Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reason-\\ning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025.\\nDavid D. Baek and Max Tegmark. Towards understanding distilled reasoning models: A\\nrepresentational approach, 2025. URL https://arxiv.org/abs/2503.03730.\\nDibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal. Thinking machines: A\\nsurvey of llm based reasoning strategies. arXiv preprint arXiv:2503.10814, 2025.\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking\\nthe capabilities of thought: A reasoning boundary framework to quantify and optimize\\nchain-of-thought. Advances in Neural Information Processing Systems, 37:54872–54904, 2024a.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang\\nHu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: A survey of long\\nchain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567,\\n2025a.\\nXinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui\\nSu, Yijie Pan, Dietrich Klakow, Wenjie Li, et al. Unveiling the key factors for distilling\\nchain-of-thought reasoning. arXiv preprint arXiv:2502.18001, 2025b.\\nYushuo Chen, Tianyi Tang, Erge Xiang, Linjiang Li, Wayne Xin Zhao, Jing Wang, Yunpeng\\nChai, and Ji-Rong Wen. Towards coarse-to-fine evaluation of inference efficiency for large\\nlanguage models. arXiv preprint arXiv:2404.11502, 2024b.\\nLi Chenglin, Qianglong Chen, Liangyue Li, Caiyu Wang, Feng Tao, Yicheng Li, Zulong\\nChen, and Yin Zhang. Mixed distillation helps smaller language models reason better. In\\nFindings of the Association for Computational Linguistics: EMNLP 2024, pp. 1673–1690, 2024.\\nYichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, and\\nHao Zhang. Efficiently serving llm reasoning programs with certaindex. arXiv preprint\\narXiv:2412.20993, 2024.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\\nXiaotian Han. Reproduce the inference time scaling exp, 2024. URL https://ahxt.github.\\nio/blog/2024-12-30-inference-time-scaling-exp/ . 2024-12-30.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low,\\nAlec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card.\\narXiv preprint arXiv:2412.16720, 2024.\\nKe Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie\\nWang, Junying Chen, Benyou Wang, et al. The first few tokens are all you need: An\\nefficient and effective unsupervised prefix fine-tuning method for reasoning models.arXiv\\npreprint arXiv:2503.02875, 2025.\\n10Preprint. Under review.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\\nlanguage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th\\nSymposium on Operating Systems Principles, 2023.\\nBespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distil-\\nlation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-\\nreasoning-distillation, 2025. Accessed: 2025-01-22.\\nAyeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-\\nthought? a token complexity approach. arXiv preprint arXiv:2503.01141, 2025.\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\\nspeculative decoding. In International Conference on Machine Learning, pp. 19274–19286.\\nPMLR, 2023.\\nDacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde,\\nKourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion\\nStoica. Llms can easily learn to reason from demonstrations structure, not content, is what\\nmatters!, 2025a. URL https://arxiv.org/abs/2502.07374.\\nYuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar\\nRamasubramanian, and Radha Poovendran. Small models struggle to learn from strong\\nreasoners. arXiv preprint arXiv:2502.12143, 2025b.\\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao,\\nHaotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A\\nsurvey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025c.\\nBaohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sa-\\nhoo, and Caiming Xiong. Reward-guided speculative decoding for efficient llm reasoning.\\narXiv preprint arXiv:2501.19324, 2025.\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,\\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv\\npreprint arXiv:2305.20050, 2023.\\nAixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr,\\nChong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and\\nefficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024.\\nRunze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and\\nBowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling,\\n2025a. URL https://arxiv.org/abs/2502.06703.\\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee,\\nand Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint\\narXiv:2503.20783, 2025b.\\nZhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D.\\nLane, and Mengwei Xu. Small language models: Survey, measurements, and insights,\\n2025. URL https://arxiv.org/abs/2409.15790.\\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,\\nLuke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple\\ntest-time scaling, 2025. URL https://arxiv.org/abs/2501.19393.\\nChien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian\\nChen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu, Ashish Singh, Yu Wang,\\nJiuxiang Gu, Franck Dernoncourt, Nesreen K. Ahmed, Nedim Lipka, Ruiyi Zhang, Xiang\\nChen, Tong Yu, Sungchul Kim, Hanieh Deilamsalehy, Namyong Park, Mike Rimer, Zhehao\\nZhang, Huanrui Yang, Ryan A. Rossi, and Thien Huu Nguyen. A survey of small language\\nmodels, 2024. URL https://arxiv.org/abs/2410.20011.\\n11Preprint. Under review.\\nAske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back.\\nReasoning with large language models, a survey. arXiv preprint arXiv:2407.11511, 2024.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\\nmodel. Advances in Neural Information Processing Systems, 36:53728–53741, 2023.\\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,\\nJulien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-\\nproof q&a benchmark. In First Conference on Language Modeling, 2024.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\\nreasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\\nGaurav Srivastava, Shuxiang Cao, and Xuan Wang. Towards reasoning ability of small\\nlanguage models. arXiv preprint arXiv:2502.11569, 2025.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient\\nreasoning for large language models. arXiv preprint arXiv:2503.16419, 2025a.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A\\nsurvey on efficient reasoning for large language models, 2025b. URL https://arxiv.org/\\nabs/2503.16419.\\nYuan Sui, Yufei He, Tri Cao, Simeng Han, and Bryan Hooi. Meta-reasoner: Dynamic\\nguidance for optimized inference-time reasoning in large language models. arXiv preprint\\narXiv:2502.19918, 2025c.\\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi\\nXu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey of reasoning with\\nfoundation models. arXiv preprint arXiv:2312.11562, 2023.\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,\\nChenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement\\nlearning with llms. arXiv preprint arXiv:2501.12599, 2025.\\nNovaSky Team. Think less, achieve more: Cut reasoning costs by 50 https://novasky-\\nai.github.io/posts/reduce-overthinking, 2025a. Accessed: 2025-01-23.\\nNovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novasky-\\nai.github.io/posts/sky-t1, 2025b. Accessed: 2025-01-09.\\nXiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yunjie Ji, Yiping Peng, Han Zhao,\\nand Xiangang Li. Think twice: Enhancing llm reasoning by scaling multi-round test-time\\nthinking, 2025. URL https://arxiv.org/abs/2503.19855.\\nYue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song,\\nDian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong\\nYu. Thoughts are all over the place: On the underthinking of o1-like llms, 2025. URL\\nhttps://arxiv.org/abs/2501.18585.\\nHeming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Con-\\ntrollable chain-of-thought compression in llms, 2025. URL https://arxiv.org/abs/2502.\\n12067.\\nTian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai\\nQiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based\\nreinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768.\\n12Preprint. Under review.\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by\\nwriting less. arXiv preprint arXiv:2502.18600, 2025.\\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\\nDayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\\narXiv:2412.15115, 2024.\\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is\\nmore for reasoning, 2025. URL https://arxiv.org/abs/2502.03387.\\nEdward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long\\nchain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan,\\nGaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement\\nlearning system at scale. arXiv preprint arXiv:2503.14476, 2025.\\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He.\\nSimplerl-zoo: Investigating and taming zero reinforcement learning for open base models\\nin the wild, 2025. URL https://arxiv.org/abs/2503.18892.\\nYongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang.\\nToken-level direct preference optimization. arXiv preprint arXiv:2404.11999, 2024.\\nJintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng,\\nHuajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression.\\narXiv preprint arXiv:2502.15589, 2025.\\nYunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae\\nLee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-\\ncorrect reasoning. arXiv preprint arXiv:2404.17140, 2024.\\n13Preprint. Under review.\\nA Appendix\\nA.1 Compuation of FLOPs\\nFLOPsprefill(s) =8sh2 + 16sh + 4s2h + 4s2n + 6shh′ + 2sh′ (2)\\nFLOPsdecode(s) =8h2 + 16h + 4sh + 4sn + 6hh′ + 2h′ (3)\\nFLOPstotal = FLOPsprefill(pl) +\\ndl −1\\n∑\\ni=0\\nFLOPsdecode(pl + i) (4)\\nWe compute the FLOPs of prefill and decoding stages based on Chen et al. (2024b); Han\\n(2024), where the batch size is 1. s is the input sequence length. h is the hidden size. h′ is the\\nintermediate size of the feed-forward network (FFN). n is the number of attention heads. d\\nis the size of each attention head, such that h = nd. pl is the length of the problem prompt.\\ndl is the number of tokens to be generated in the solution.\\n�1, �1 �2, �2\\n�3\\n�1, �1 �2, �2\\n�1, �1 �2, �2 �3, �3\\nDecode Prefix\\n�3\\n�4 �4\\nkv cache\\n�1, �1\\n�2, �2\\n�3\\n �4\\n�1, �1\\n�2, �2\\n�3\\n�1, �1\\n�2, �2\\n�3, �3\\n�4\\nDecode\\nPrefix\\n   kv \\ncache\\n(a) decode v.s. prefix\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n2\\n4\\n6\\n8Time (seconds)\\ndecode\\nprefix (b) Deepseek-1.5B\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n5\\n10\\n15\\n20Time (seconds)\\ndecode\\nprefix (c) Deepseek-32B\\nFigure 7: Comparison between Decode and Prefix stages: average time consumed by the\\n1.5B and 32B models when generating different numbers of output tokens. As the number\\nincreases, decoding time grows significantly, while prefix time remains nearly constant.\\nA.2 Hyperparameters of Speculative Thinking\\nA sentence is labeled Affirmation or Reflection if it contains affirmation cues (e.g., yes, yep)\\nor backtracking cues (e.g., wait, alternatively); and Statement if neither type is present. If\\nboth Affirmation and Reflection keywords appear, the decision is made based on majority\\ncount, and in case of a tie, we default to Reflection.\\nWithin the proposed framework, we define three sets of indicative keywords that trigger\\ndifferent forms of target model intervention:\\n• Reflection keywords, used to detect reflection or hesitation: “wait”, “alternatively”,\\n“hold on”, “another”, “verify”, “think again”, “recap”, “check”.\\n• Affirmation keywords, indicating confidence or commitment to a line of reasoning:\\n“yeah”, “yes”, “final answer”, “confident”.\\n• Verification keywords, used to trigger verification-based intervention: “verify”,\\n“think again”, “recap”, “check”.\\nWe also configure fixed token lengths for the target model’s interventions in different\\nscenarios: n1 = 20 for Affirmation/Reflection Takeover,n2 = 125 for Verification Takeover,\\nand n3 = 125 for Excessive Negativity Takeover. These hyperparameters are selected to\\nbalance informativeness and computational cost.\\nA.3 Results of Deepseek-Distilled Qwen-2.5-7B\\nWe present the accuracy and average output length of Deepseek-Distilled Qwen-2.5-7B on\\nfour datasets.\\n14Preprint. Under review.\\n7B 7B+32B 32B\\n20\\n40\\n60Accuracy\\n48.89 53.33\\n65.56\\n7B 7B+32B 32B\\n11000\\n12000\\n13000\\n14000Average Length\\n13250 13214\\n12274\\n(a) AIME\\n7B 7B+32B 32B\\n90\\n91\\n92\\n93\\n94Accuracy\\n92.80 93.00 92.80\\n7B 7B+32B 32B\\n3600\\n3800\\n4000Average Length\\n3975\\n3768 3802 (b) MATH500\\n7B 7B+32B 32B\\n30\\n40\\n50\\n60Accuracy\\n45.45\\n52.02\\n61.62\\n7B 7B+32B 32B\\n5000\\n5500\\n6000Average Length\\n6111\\n5952\\n5407\\n(c) GPQA\\n7B 7B+32B 32B\\n90\\n92\\n94\\n96Accuracy\\n92.50 92.50\\n95.00\\n7B 7B+32B 32B\\n5000\\n6000\\n7000Average Length\\n6094\\n5116\\n7107 (d) AMC23\\nFigure 8: Accuracy and average output length of models on four datasets (AIME 2020–2024,\\nMATH500, GPQA, and AMC23). 1B denotes Deepseek-Distilled Qwen 2.5-7B model, 32B\\nrefers to Deepseek-Distilled Qwen 2.5-32B model, and 7B+32B represents Speculative Think-\\ning, where 32B model assists 7B model. Speculative Thinking leads to a significant improve-\\nment in the 7B model’s accuracy while effectively reducing its output length.\\nA.4 Proportion of Top-10 Preceding Tokens\\nTable 4: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in the\\nMATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-1.5B model.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.708) \" \" (0.207) \" \"(0.055) \").\\\\n\\\\n\"(0.011) \"?\\\\n\\\\n\"(0.008)\\n\" \\\\n\\\\n\"(0.004) \"\\\\n\\\\n\"(0.003) \")\\\\n\\\\n\"(0.001) \":\\\\n\\\\n\"(0.001) \" )\\\\n\\\\n\"(0.001)\\nhmm \" \" (0.689) \".\\\\n\\\\n\"(0.139) \")\\\\n\\\\n\"(0.043) \"]\\\\n\\\\n\"(0.037) \"\\\\n\\\\n\"(0.033)\\n\").\\\\n\\\\n\"(0.027) \" \" (0.007) \"]\\\\n\"(0.007) \"?\\\\n\\\\n\"(0.004) \" \\\\n\\\\n\"(0.004)\\nwait \".\\\\n\\\\n\" (0.647) \" \"(0.230) \"?\\\\n\\\\n\"(0.044) \").\\\\n\\\\n\"(0.026) \"\\\\n\\\\n\"(0.016)\\n\")\\\\n\\\\n\"(0.009) \"]\\\\n\\\\n\"(0.007) \" \\\\n\\\\n\"(0.005) \" \" (0.004) \":\\\\n\\\\n\"(0.002)\\nTable 5: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in the\\nMATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-7B model.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.929) \" \"(0.048) \"?\\\\n\\\\n\"(0.008) \").\\\\n\\\\n\"(0.007) \" \\\\n\\\\n\"(0.004)\\n\")\\\\n\\\\n\"(0.001) \"?)\\\\n\\\\n\"(0.001) \"].\\\\n\\\\n\"(0.000) \"]\\\\n\\\\n\"(0.000) \"’.\\\\n\\\\n\"(0.000)\\nhmm \" \" (0.697) \".\\\\n\\\\n\"(0.123) \"\\\\n\\\\n\"(0.047) \")\\\\n\\\\n\"(0.043) \"]\\\\n\\\\n\"(0.038)\\n\").\\\\n\\\\n\"(0.025) \"?\\\\n\\\\n\"(0.006) \" \\\\n\\\\n\"(0.005) \"]\\\\n\"(0.003) \" )\\\\n\\\\n\"(0.003)\\nwait \".\\\\n\\\\n\" (0.637) \" \"(0.224) \"?\\\\n\\\\n\"(0.048) \").\\\\n\\\\n\"(0.029) \"\\\\n\\\\n\"(0.019)\\n\")\\\\n\\\\n\"(0.015) \" \\\\n\\\\n\"(0.007) \"]\\\\n\\\\n\"(0.005) \":\\\\n\\\\n\"(0.004) \" )\\\\n\\\\n\"(0.002)\\n15Preprint. Under review.\\nTable 6: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in the\\nMATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-14B model.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.867) \" \"(0.076) \").\\\\n\\\\n\"(0.022) \"?\\\\n\\\\n\"(0.015) \" \\\\n\\\\n\"(0.013)\\n\")\\\\n\\\\n\"(0.001) \"\\\\n\\\\n\"(0.001) \"]\\\\n\\\\n\"(0.001) \"].\\\\n\\\\n\"(0.001) \" \" (0.001)\\nhmm \" \" (0.649) \".\\\\n\\\\n\"(0.159) \"\\\\n\\\\n\"(0.047) \")\\\\n\\\\n\"(0.036) \"]\\\\n\\\\n\"(0.033)\\n\").\\\\n\\\\n\"(0.033) \" \\\\n\\\\n\"(0.010) \"?\\\\n\\\\n\"(0.009) \"]\\\\n\"(0.007) }\\\\n\\\\n(0.004)\\nwait \".\\\\n\\\\n\" (0.643) \" \"(0.206) \"?\\\\n\\\\n\"(0.053) \").\\\\n\\\\n\"(0.032) \"\\\\n\\\\n\"(0.021)\\n\" \\\\n\\\\n\"(0.015) \")\\\\n\\\\n\"(0.013) \"]\\\\n\\\\n\"(0.004) \":\\\\n\\\\n\"(0.003) \"?)\\\\n\\\\n\"(0.001)\\nA.5 Statistics of Different Size model\\n1.5B 7B 32B\\n80\\n85\\n90\\n95Accuracy (%)\\n83.20\\n92.80 92.80\\n1.5B 7B 32B\\n4000\\n5000\\n6000Avg T oken Num\\n5439\\n3975 3802\\n1.5B 7B 32B\\n0\\n5000\\n10000\\n15000Average Number of T okens\\n#=416\\n#=84\\n#=464\\n#=36\\n#=464\\n#=36\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n4000\\n6000\\n8000\\n10000T otal Wait Count\\n7936\\n10019\\n6896\\n4583\\n5680\\n6627\\nCorrect Wait Count Wrong Wait Count\\nFigure 9: Accuracy and output statistics of three models on the MATH500 dataset.\\n1.5B 7B 32B\\n30\\n40\\n50\\n60\\n70Accuracy (%)33.84\\n45.45\\n61.62\\n1.5B 7B 32B\\n5000\\n6000\\n7000\\n8000Avg T oken Num\\n7921\\n6111\\n5406\\n1.5B 7B 32B\\n0\\n2000\\n4000\\n6000\\n8000\\n10000Average Number of T okens\\n#=67\\n#=131\\n#=90\\n#=108\\n#=122\\n#=76\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n4000\\n6000\\n8000\\n10000T otal Wait Count\\n4878\\n8990\\n3087\\n5792\\n3881\\n4591\\nCorrect Wait Count Wrong Wait Count\\nFigure 10: Accuracy and output statistics of three models on the GPQA dataset.\\n16Preprint. Under review.\\n1.5B 7B 32B\\n70\\n80\\n90\\n100Accuracy (%)\\n75.00\\n92.50 95.00\\n1.5B 7B 32B\\n6000\\n8000\\n10000\\n12000Avg T oken Num\\n10460\\n6093\\n7106\\n1.5B 7B 32B\\n0\\n10000\\n20000\\n30000Average Number of T okens\\n#=30\\n#=10\\n#=37\\n#=3\\n#=38\\n#=2\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n0\\n500\\n1000\\n1500\\n2000T otal Wait Count\\n1088\\n1470\\n887\\n611\\n974\\n772\\nCorrect Wait Count Wrong Wait Count\\nFigure 11: Accuracy and output statistics of three models on the AMC23 dataset.\\nA.6 Results of Non-reasoning model\\nTable 7: Accuracy, average output length, and estimated speed on four datasets. 1B-Instruct\\nrefers to Qwen-2.5-1.5B. “+” means with the help of reasoning models. Modify ratio indicates\\nthe proportion of tokens in the final output that come from target model. After applying\\nSpeculative Thinking, 1B-Instruct models demonstrate improvements in accuracy\\ndataset speculative target avg modify estimated acc\\npass@1 model model length ratio speed (%) Improv.\\nAIME 1B-Instruct\\nnormal 1701.5 – 224.4 4.4 –\\n+7B 14240.7 37.0% 76.9 8.9 +102.3%\\n+32B 15536.7 34.0% 51.6 10.0 +127.3%\\nGPQA 1B-Instruct\\nnormal 694.9 – 164.9 23.7 –\\n+7B 9019.3 26.0% 95.4 30.3 +27.8%\\n+32B 10500.2 26.0% 62.4 33.3 +40.5%\\nMATH500 1B-Instruct\\nnormal 1424.1 – 205.4 50.2 –\\n+7B 7947.2 30.0% 58.7 48.8 -2.9%\\n+32B 8935.7 29.0% 89.7 48.2 -4.0%\\nAMC23 1B-Instruct\\nnormal 1605.0 – 217.6 20.0 –\\n+7B 19376.5 23.0% 89.2 27.5 +37.5%\\n+32B 17114.4 23.0% 65.4 30.0 +50.0%\\n17'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    text += docs[i].page_content\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcacc24d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      2\u001b[39m splitter = RecursiveCharacterTextSplitter(chunk_size=\u001b[32m1000\u001b[39m, chunk_overlap=\u001b[32m200\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m chunks = splitter.split_text(\u001b[43mtext\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09969b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = [Document(page_content=item) for item in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba456df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d9eb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6293639",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key = os.environ['GOOGLE_API_KEY'])\n",
    "vector_store = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de2456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7539a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dbd8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(template=\"\"\"\n",
    "You are a helpful assistant.\n",
    "Answer ONLY from the provided transcript context.\n",
    "If the context is insufficient, just say you don't know.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "                        \"\"\", input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a902fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "    # Handle both list[Document] and single Document\n",
    "    if not isinstance(retrieved_docs, list):\n",
    "        retrieved_docs = [retrieved_docs]\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "    \"context\": retriever | RunnableLambda(format_docs),\n",
    "    \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "normal_chain = parallel_chain | prompt | model | parser\n",
    "main_chain = parallel_chain | normal_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b46bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "# parallel_chain produces {\"context\": str, \"question\": str}\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"context\": retriever | RunnableLambda(format_docs),\n",
    "    \"question\": RunnablePassthrough()\n",
    "})\n",
    "\n",
    "parser = StrOutputParser()\n",
    "normal_chain = prompt | model | parser   # expects {\"context\", \"question\"} and gives string\n",
    "\n",
    "# Wrap to include both answer + raw retrieved docs\n",
    "\n",
    "\n",
    "def with_docs(inputs):\n",
    "    # raw retrieval\n",
    "    raw_docs = retriever.invoke(inputs[\"question\"])\n",
    "\n",
    "    # force convert list[Document] -> string\n",
    "    formatted_context = \"\\n\\n\".join([doc.page_content for doc in raw_docs])\n",
    "\n",
    "    # run prompt + model\n",
    "    answer = normal_chain.invoke({\n",
    "        \"context\": formatted_context,\n",
    "        \"question\": inputs[\"question\"]\n",
    "    })\n",
    "\n",
    "    # return both answer + contexts (list of str for Ragas)\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": [doc.page_content for doc in raw_docs]\n",
    "    }\n",
    "\n",
    "main_chain = RunnableLambda(with_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "527f6c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The document is about comparing Decode and Prefix stages, Speculative Thinking, and Speculative Decoding.',\n",
       " 'contexts': ['�3\\n�1, �1\\n�2, �2\\n�3, �3\\n�4\\nDecode\\nPrefix\\n   kv \\ncache\\n(a) decode v.s. prefix\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n2\\n4\\n6\\n8Time (seconds)\\ndecode\\nprefix (b) Deepseek-1.5B\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n5\\n10\\n15\\n20Time (seconds)\\ndecode\\nprefix (c) Deepseek-32B\\nFigure 7: Comparison between Decode and Prefix stages: average time consumed by the\\n1.5B and 32B models when generating different numbers of output tokens. As the number\\nincreases, decoding time grows significantly, while prefix time remains nearly constant.\\nA.2 Hyperparameters of Speculative Thinking\\nA sentence is labeled Affirmation or Reflection if it contains affirmation cues (e.g., yes, yep)\\nor backtracking cues (e.g., wait, alternatively); and Statement if neither type is present. If\\nboth Affirmation and Reflection keywords appear, the decision is made based on majority\\ncount, and in case of a tie, we default to Reflection.',\n",
       "  'or stronger than the target model may lead to suboptimal or even detrimental outcomes.\\n4.3 Comparisons between Speculative Decoding and Speculative Thinking\\nMATH500 AIME\\n60\\n80\\n100Accuracy (%)\\n92.8\\n65.6\\n93.0\\n58.9\\n93.0\\n53.3\\n32B\\nDecoding\\nThinking\\nMATH500 AIME\\n25\\n30\\n35\\n40\\n45\\n50Speed (token/s)\\n30.0\\n32.230.4 28.7\\n46.0\\n41.0\\nFigure 6: Comparison between Speculative Decoding and Thinking using a 7B speculative\\nmodel and a 32B target model. In Speculative Decoding, speculative model generates 20\\ntokens per step to match the number of intervention tokens in Speculative Thinking.\\nThis experiment primarily compares the differences between speculative decoding and\\nspeculative thinking. Due to the constraint that speculative decoding requires the specula-\\ntive model and the target model to have the same vocabulary size, we obtain speculative\\ndecoding results where the speculative model is 7B, and the target model is 32B. To align',\n",
       "  'takes over speculative model’s generation under the following three scenarios. The hyper-\\nparameters for Speculative Thinking—such as the selection of Reflection and Affirmation\\nkeywords, and the values of control parameters n1, n2, and n3 are shown in Appendix A.2.\\n(1) Affirmation/Reflection Takeover. This mechanism leverages stronger reasoning ability\\nof target model to help speculative model decide whether to continue or revise. Speculative\\nmodel first generates responses until a delimiter token (e.g., \\\\n\\\\n) is encountered. After this\\ndelimiter, speculative model generates one full sentence (i.e., n1 tokens). We then classify\\nthe sentence into three situations: Affirmation, Reflection, or Statement, based on keyword\\nmatching, as shown in Appendix A.2. If speculative model’s sentence is classified as either\\nAffirmation or Reflection, target model immediately takes over and generates n1 tokens.\\nSpeculative model then resumes generation conditioned on target model’s output.',\n",
       "  '\\\\boxed{}.”.\\nReferences\\nSimon A Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reason-\\ning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025.\\nDavid D. Baek and Max Tegmark. Towards understanding distilled reasoning models: A\\nrepresentational approach, 2025. URL https://arxiv.org/abs/2503.03730.\\nDibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal. Thinking machines: A\\nsurvey of llm based reasoning strategies. arXiv preprint arXiv:2503.10814, 2025.\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking\\nthe capabilities of thought: A reasoning boundary framework to quantify and optimize\\nchain-of-thought. Advances in Neural Information Processing Systems, 37:54872–54904, 2024a.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang\\nHu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: A survey of long']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke({\"question\": \"What is the document about?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e2b8",
   "metadata": {},
   "source": [
    "## Now making the eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8e3ce07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Preprint. Under review.\\nSpeculative Thinking: Enhancing Small-Model Reasoning\\nwith Large Model Guidance at Inference Time\\nWang Yang1, Xiang Yue2, Vipin Chaudhary1, Xiaotian Han1\\n1Case Western Reserve University 2Carnegie Mellon University\\n{wxy320,vxc204,xhan}@case.edu xyue2@andrew.cmu.edu\\nAbstract\\nRecent advances leverage post-training to enhance model reasoning perfor-\\nmance, which typically requires costly training pipelines and still suffers\\nfrom inefficient, overly lengthy outputs. We introduce Speculative Think-\\ning1, a training-free framework that enables large reasoning models to\\nguide smaller ones during inference at the reasoning level, distinct from\\nspeculative decoding, which operates at the token level. Our approach\\nis based on two observations: (1) reasoning-supportive tokens such as\\n“wait” frequently appear after structural delimiters like “\\\\n\\\\n”, serving as\\nsignals for reflection or continuation; and (2) larger models exhibit stronger'),\n",
       " Document(metadata={}, page_content='“wait” frequently appear after structural delimiters like “\\\\n\\\\n”, serving as\\nsignals for reflection or continuation; and (2) larger models exhibit stronger\\ncontrol over reflective behavior, reducing unnecessary backtracking while\\nimproving reasoning quality. By strategically delegating reflective steps\\nto a more capable model, our method significantly boosts the reasoning\\naccuracy of reasoning models while shortening their output. With the assis-\\ntance of the 32B reasoning model, the 1.5B model’s accuracy on MATH500\\nincreases from 83.2% to 89.4%, marking a substantial improvement of 6.2%.\\nSimultaneously, the average output length is reduced from5439 tokens to\\n4583 tokens, representing a 15.7% decrease. Moreover, when applied to a\\nnon-reasoning model (Qwen-2.5-7B-Instruct), our framework boosts its\\naccuracy from 74.0% to 81.8% on the same benchmark, achieving a relative\\nimprovement of 7.8%.\\n+6.7%\\n-11.8%\\n(a) AIME\\n+6.2% -15.7% (b) MATH500\\n+8.1%\\n-4.0%\\n(c) GPQA\\n+5.0% -16.9% (d) AMC23'),\n",
       " Document(metadata={}, page_content='accuracy from 74.0% to 81.8% on the same benchmark, achieving a relative\\nimprovement of 7.8%.\\n+6.7%\\n-11.8%\\n(a) AIME\\n+6.2% -15.7% (b) MATH500\\n+8.1%\\n-4.0%\\n(c) GPQA\\n+5.0% -16.9% (d) AMC23\\nFigure 1: Speculative Thinking significantly improves the 1.5B model’s reasoning accuracy\\nwhile simultaneously reducing its average output length. This figure compares the accuracy\\nand average output length of models on four mathematical and reasoning datasets, includ-\\ning AIME 2020–2024, MATH500, GPQA, and AMC23. \"1.5B\" denotes the Deepseek-Distilled\\nQwen 2.5-1.5B model, \"32B\" refers to the Deepseek-Distilled Qwen 2.5-32B model, and\\n\"1.5B+32B\" represents our proposed Speculative Thinking method, where the 32B model\\nsupervises reflective reasoning steps of the 1.5B model during inference.\\n1Our code is available at https://github.com/uservan/speculative_thinking\\n1\\narXiv:2504.12329v1  [cs.CL]  12 Apr 2025Preprint. Under review.\\n1 Introduction'),\n",
       " Document(metadata={}, page_content='1Our code is available at https://github.com/uservan/speculative_thinking\\n1\\narXiv:2504.12329v1  [cs.CL]  12 Apr 2025Preprint. Under review.\\n1 Introduction\\nSmaller language models are widely used in real-world applications due to their lower com-\\nputational and memory requirements (Nguyen et al., 2024; Lu et al., 2025; Sui et al., 2025b).\\nHowever, they often underperform on tasks requiring complex reasoning (Li et al., 2025b;\\nSrivastava et al., 2025; Liu et al., 2025a). Improving their capabilities involves extensive\\npost-training such as supervised fine-tuning on high-quality reasoning traces (Chenglin\\net al., 2024) or reinforcement learning with verifiable signals (Shao et al., 2024; Chen et al.,\\n2025a; Zhang et al., 2024), which can be costly, data-intensive, and difficult to scale.\\nTo avoid retraining, inference-time scaling methods have been proposed to elicit better\\nintermediate steps from small models (Sui et al., 2025c; Xu et al., 2025). While lightweight'),\n",
       " Document(metadata={}, page_content='To avoid retraining, inference-time scaling methods have been proposed to elicit better\\nintermediate steps from small models (Sui et al., 2025c; Xu et al., 2025). While lightweight\\nand training-free, these approaches depend entirely on the model’s existing abilities and\\noften yield limited or inconsistent improvements, particularly on complex tasks Li et al.\\n(2025b). Larger models, by contrast, exhibit significantly stronger reasoning abilities across\\na wide range of benchmarks (Muennighoff et al., 2025; Ye et al., 2025; Plaat et al., 2024), but\\ntheir inference cost and latency make them impractical for many deployment scenarios. This\\ntension motivates a central question: Can we improve small reasoning models during inference\\nby selectively leveraging large models, without additional training?\\nInspired by speculative decoding (Leviathan et al., 2023), which accelerates generation\\nby using a small model to propose tokens later verified by a larger model, we propose'),\n",
       " Document(metadata={}, page_content='Inspired by speculative decoding (Leviathan et al., 2023), which accelerates generation\\nby using a small model to propose tokens later verified by a larger model, we propose\\nSpeculative Thinking, a training-free framework for improving small-model reasoning\\nduring inference. Unlike speculative decoding, which operates at the token level, our\\napproach focuses on reasoning level. A small model generates most of the output but\\nselectively hands off difficult reasoning segments to a stronger model. These segments\\nare identified through structural cues—such as paragraph breaks (“\\\\n\\\\n”) followed by\\nreflective phrases like “wait” and “alternatively”—which often mark internal revision. Small\\nmodels frequently struggle in these cases, producing verbose outputs, while larger models\\nare more concise and effective at backtracking. By dynamically detecting these points and\\ndelegating them to a large mentor model, Speculative Thinking preserves the small model’s'),\n",
       " Document(metadata={}, page_content='are more concise and effective at backtracking. By dynamically detecting these points and\\ndelegating them to a large mentor model, Speculative Thinking preserves the small model’s\\nefficiency while leveraging the large model’s strength exactly where it matters most.\\nEmpirical results demonstrate the effectiveness of this hybrid approach. A 1.5B model\\nassisted by Deepseek-distilled Qwen-2.5-32B improves by +6.6% on AIME , +6.2% on\\nMATH500 (Lightman et al., 2023), +8.1% onGPQA (Rein et al., 2024), and +5.0% onAMC23 ,\\nwhile reducing output length—indicating more efficient reasoning. Notably, this approach\\nis also effective for models not explicitly trained for reasoning: Qwen-2.5-7B-Instructgains\\n+7.8% on MATH500 and +14.2% on GPQA when assisted by the 32B mentor.\\nIn summary, Speculative Thinking offers a new inference-time paradigm that fuses the\\nefficiency of small models with the reasoning strength of large models. It opens a promising'),\n",
       " Document(metadata={}, page_content='In summary, Speculative Thinking offers a new inference-time paradigm that fuses the\\nefficiency of small models with the reasoning strength of large models. It opens a promising\\npath toward cost-effective reasoning augmentation for real-world inference.\\n2 Motivations\\n2.1 Analysis of LLM Reasoning Process\\nThis section investigates characteristic patterns that commonly emerge during the reasoning\\nprocesses of current reasoning models. By analyzing these patterns, we aim to uncover\\npotential avenues for enhancing and optimizing the models’ reasoning capabilities.\\n“\\\\n\\\\n” acts as a structural clue in model reasoning process. During inference, reasoning\\nmodels frequently generate certain reasoning-supportive tokens such as “wait”, “hmm” and\\n“alternatively”, which are relative with the model’s self-reflection behavior. To further ana-\\nlyze them, we examine the preceding token distribution for reasoning-supportive tokens in'),\n",
       " Document(metadata={}, page_content=\"“alternatively”, which are relative with the model’s self-reflection behavior. To further ana-\\nlyze them, we examine the preceding token distribution for reasoning-supportive tokens in\\nDeepseek-distilled Qwen-2.5-32B on the MATH500 dataset. As shown in Table 1, we report\\nthe top 10 most frequent preceding tokens for three representative reasoning-supportive\\ntokens: “wait”, “alternatively”, and “hmm”. Notably, for all three tokens, the preceding\\ntoken is overwhelmingly dominated by the newline symbol “\\\\n\\\\n“. For instance, in the\\n2Preprint. Under review.\\nOkay, so I need to convert the rectangular coordinates.... \\\\n\\\\n First, let's recall the formulas for converting \\nrectangular…\\\\n\\\\n Wait, but in this case, the x-coordinate is 0…. \\\\n\\\\n Now, for the angle θ, since x is 0 and y is \\npositive…. \\\\n\\\\n Let me verify that. If x is 0, then the point is …\\\\n\\\\n Is there a chance I made a mistake with\"),\n",
       " Document(metadata={}, page_content=\"positive…. \\\\n\\\\n Let me verify that. If x is 0, then the point is …\\\\n\\\\n Is there a chance I made a mistake with \\nthe formula? Hmm, the formula ….\\\\n\\\\n Let me think of another way to remember it. ….\\\\n\\\\n I also know that in \\npolar coordinates….\\\\n\\\\n Wait a second, just to be thorough, let's compute θ using the arctangent. ….\\\\n\\\\n Yeah, \\nso r is 3 and θ is π/2. So the polar coordinates should be (3, π/2).\\\\n\\\\n Wait, let me also make sure that …\\\\n\\\\nSo, \\nI think I'm confident that the polar coordinates for (0,3) are (3, π/2).\\nReflection AffirmationDeepseek-distilled Qwen-2.5-1.5B Statement\\nTo convert the point $(0, 3)$ from rectangular coordinates….\\\\n\\\\n ### Step 1: Calculate the Radius… \\\\n\\\\n For the point \\n$(0, 3)$:\\\\n\\\\\\\\[ r = \\\\\\\\sqrt{0^2 + 3^2} = \\\\\\\\sqrt{9} = 3 \\\\\\\\]...\\\\n\\\\n ### Step 2: Calculate the….\\\\n\\\\nFor the point $(0, 3)$:\\\\n- The \\nx-coordinate is $0$....\\\\n\\\\n This point lies on the positive y-axis. …\\\\n\\\\n ### Step 3: Final Answer\\\\n Combining the values\"),\n",
       " Document(metadata={}, page_content='x-coordinate is $0$....\\\\n\\\\n This point lies on the positive y-axis. …\\\\n\\\\n ### Step 3: Final Answer\\\\n Combining the values \\nwe …\\\\n\\\\n Thus, the point $(0, 3)$ in rectangular coordinates can be represented as:\\\\n\\\\\\\\[\\\\n\\\\\\\\boxed{(3, \\\\\\\\frac{\\\\\\\\pi}{2})}\\\\n\\\\\\\\]\\nReflection AffirmationStatement\\n Qwen-2.5-1.5B-Instruct\\nFigure 2: Comparison of outputs between Reasoning Model and Non-reasoning model.\\nReasoning models often generate negative sentences—typically containing tokens such as\\n“wait”—immediately following the delimiter \"\\\\n\\\\n\". These sentences serve as reflective\\nprompts, helping the model to backtrack, reassess, and verify prior reasoning steps.\\nTable 1: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in\\nthe MATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-32B model. We\\nfind that over 80% of reasoning-supportive tokens appear after the occurrence of ”\\\\n\\\\n”,\\nindicating that it plays a crucial role in triggering reflective behavior during reasoning.'),\n",
       " Document(metadata={}, page_content='find that over 80% of reasoning-supportive tokens appear after the occurrence of ”\\\\n\\\\n”,\\nindicating that it plays a crucial role in triggering reflective behavior during reasoning.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.928) \" \"(0.050) \").\\\\n\\\\n\"(0.007) \"?\\\\n\\\\n\"(0.006) \" \\\\n\\\\n\"(0.004)\\n\"].\\\\n\\\\n\"(0.002) \"\\\\n\\\\n\"(0.001) \")\\\\n\\\\n\"(0.001) \"]\\\\n\\\\n\"(0.001) \"?)\\\\n\\\\n\"(0.001)\\nhmm \" \"(0.690) \".\\\\n\\\\n\"(0.131) \"\\\\n\\\\n\"(0.044) \")\\\\n\\\\n\"(0.038) \").\\\\n\\\\n\"(0.035)\\n\"]\\\\n\\\\n\"(0.029) \" \\\\n\\\\n\"(0.009) \"?\\\\n\\\\n\"(0.007) \"?)\\\\n\\\\n\"(0.002) \"?\"\\\\n\\\\n\"(0.002)\\nwait \".\\\\n\\\\n\"(0.699) \" \"(0.182) \"?\\\\n\\\\n\"(0.039) \").\\\\n\\\\n\"(0.022) \"\\\\n\\\\n\"(0.017)\\n\")\\\\n\\\\n\"(0.011) \"]\\\\n\\\\n\"(0.007) \" \\\\n\\\\n\"(0.007) \":\\\\n\\\\n\"(0.004) \"].\\\\n\\\\n\"(0.002)\\ncase of “wait”, over 80% of its preceding tokens are “\\\\n\\\\n“. This strongly suggests that\\n“\\\\n\\\\n“ acts as a thinking cue—prompting the model to decide whether to reflect on the\\nprevious thought or proceed with the current line of reasoning. We have also extend this'),\n",
       " Document(metadata={}, page_content='“\\\\n\\\\n“ acts as a thinking cue—prompting the model to decide whether to reflect on the\\nprevious thought or proceed with the current line of reasoning. We have also extend this\\nsame analysis to other models on the MATH500 dataset in Appendix A.4.\\nCase analysis of LLM reasoning process to prove the role of \"\\\\n\\\\n\". To further prove the\\neffect of \"\\\\n\\\\n\", we conduct a case study on responses generated by Deepseek-distilled\\nQwen-2.5-1.5B and Qwen-2.5-1.5B-Instruct when answering questions in Figure 2. Specifi-\\ncally, we treat each occurrence of\"\\\\n\\\\n\"as a delimiter to segment the model’s output into\\nmultiple parts. We then categorize each segment as Affirmation, Reflection, or Statement:\\nAffirmation segments include affirming expressions such as yeah or yes, indicating a contin-\\nuation or endorsement of the preceding thought; Reflection segments contain expressions\\nlike wait, alternatively, or hmm, signaling the model’s intent to reflect its previous thought;'),\n",
       " Document(metadata={}, page_content='uation or endorsement of the preceding thought; Reflection segments contain expressions\\nlike wait, alternatively, or hmm, signaling the model’s intent to reflect its previous thought;\\nStatement segments often corresponding to formulaic expressions or factual outputs. Empir-\\nical analysis of representative examples in Figure 2 shows that the first sentence after each\\n”\\\\n\\\\n”often contains reasoning-related cues. This suggests that ”\\\\n\\\\n”acts as a discourse\\nmarker, prompting the model either affirm, reflect or state the previous thought.\\n2.2 Comparisons between Small and Large Reasoning Models\\nIn this section, we compare reasoning models of different sizes to find the differences\\nbetween small and large reasoning models, including Deepseek-distilled Qwen-2.5-32B, 7B,\\nand 1.5B. Specifically, we analyze their performance differences in terms of accuracy and\\noutput length on the AIME 2022-2024 dataset. All the results are shown in Figure 3 and the'),\n",
       " Document(metadata={}, page_content='and 1.5B. Specifically, we analyze their performance differences in terms of accuracy and\\noutput length on the AIME 2022-2024 dataset. All the results are shown in Figure 3 and the\\ndetailed statistics on other datasets can be found in Appendix A.5.\\n3Preprint. Under review.\\n1.5B 7B 32B\\n20\\n40\\n60Accuracy (%)25.56\\n48.89\\n65.56\\n1.5B 7B 32B\\n12000\\n14000\\n16000\\n18000Avg T oken Num\\n17798\\n13250\\n12274\\n1.5B 7B 32B\\n0\\n5000\\n10000\\n15000\\n20000\\n25000Average Number of T okens\\n#=23\\n#=67\\n#=44\\n#=46\\n#=59\\n#=31\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n2500\\n5000\\n7500\\n10000T otal Wait Count1078\\n10760\\n1838\\n5989\\n2377\\n4365\\nCorrect Wait Count Wrong Wait Count\\nFigure 3: Accuracy and output statistics of three models on the AIME 2022–2024 dataset.\\nReported metrics include: overall accuracy (upper left), average output length (upper right),\\naverage output length (down left) for correct and incorrect answers, as well as the number'),\n",
       " Document(metadata={}, page_content='Reported metrics include: overall accuracy (upper left), average output length (upper right),\\naverage output length (down left) for correct and incorrect answers, as well as the number\\nof reflective sentences—such as those containing terms like “wait” or “alternatively”—in\\nboth correct and incorrect responses (down right). “#=67” indicates the number of incorrect\\nresponses made by the 1.5B model is 67. The average output length of small models is\\nsignificantly higher than that of large models. This is primarily due to the excessive length\\nof incorrect responses. At its core, this phenomenon stems from inefficient and redundant\\nself-reflection in small models, which often leads to failed reasoning attempts and ultimately\\nprevents them from arriving at correct answers before its max output length.\\nSmall reasoning models have worse reasoning performances and much longer responses.\\nWe first report the accuracy and average output length for all three models. As shown'),\n",
       " Document(metadata={}, page_content='Small reasoning models have worse reasoning performances and much longer responses.\\nWe first report the accuracy and average output length for all three models. As shown\\nin Figure 3, smaller models exhibit significantly lower accuracy compared to larger ones.\\nInterestingly, the average output length of smaller models tends to be much longer. As\\nmodel size increases, accuracy improves while outputs become more concise. To further\\nunderstand this phenomenon, we analyze the average lengths of correct and incorrect\\nresponses separately. We find that, across all model sizes, incorrect responses are consistently\\nmuch longer than correct ones. This suggests that the overall average output length is\\nheavily influenced by the proportion of incorrect answers, which are typically more verbose.\\nLarger-scale models exhibit more effective self-reflection and backtracking during reason-\\ning. To further investigate why incorrect responses are substantially longer than correct ones,'),\n",
       " Document(metadata={}, page_content='Larger-scale models exhibit more effective self-reflection and backtracking during reason-\\ning. To further investigate why incorrect responses are substantially longer than correct ones,\\nwe analyze the frequency of reflective phrases—such as“wait” and “alternatively”—which in-\\ndicate hesitation, self-reflection, or backtracking in reasoning process. As shown in Figure 3,\\nsuch phrases occur far more frequently in incorrect responses, particularly in smaller models.\\nThis suggests that smaller models tend to over-reflect yet under-reason, leading to inefficient\\nexploration of the solution space. Consequently, the excessive length of their outputs is\\nprimarily due to their inability to converge on correct answers within the maximum context\\nwindow, resulting in repetitive branching and redundant verification steps.\\n2.3 How to Combine Small and Large Reasoning Model?\\nWe observe that when reasoning models generate incorrect answers, their average output'),\n",
       " Document(metadata={}, page_content='2.3 How to Combine Small and Large Reasoning Model?\\nWe observe that when reasoning models generate incorrect answers, their average output\\nlength increases significantly. A key manifestation of this is the overuse of words like “wait”,\\nindicating excessive self-reflection and backtracking. However, as model size increases,\\nsuch reflection becomes more efficient, resulting in fewer redundant revisions and shorter\\noutputs overall. This naturally raises an intriguing question: Can the reasoning ability of\\nlarger models be leveraged to monitor smaller models during inference?\\nWe propose a novel intervention strategy that utilizes the \"\\\\n\\\\n\" reasoning pattern as a\\ncontrol point for collaborative inference. In particular, when a smaller model encounters a\\n\"\\\\n\\\\n\"followed by tokens like ”wait”, which often signal confusion or indecision, we can\\ndelegate the subsequent reasoning step to a larger model because the larger one could give\\n4Preprint. Under review.\\nLarge Model (Target Model)'),\n",
       " Document(metadata={}, page_content='delegate the subsequent reasoning step to a larger model because the larger one could give\\n4Preprint. Under review.\\nLarge Model (Target Model)\\nSmall Model (Speculative Model)\\nGuiding\\nThinking\\nFinal Answer is 706.   Convert the point...\\nOkay, so I need …  Wait, but maybe …\\n   So, yep, I think …Wait, seems it is…\\nSo, it is correct … Double-check it:\\nThe first step is …\\n   Convert the point...\\nFigure 4: Overview of speculative thinking. A small model generates most output but\\nselectively delegates challenging segments—marked by structural cues such as paragraph\\nbreaks (“\\\\n\\\\n”) followed by reflective phrases like “wait,” “alternatively,” or “hold on”—to\\na stronger model. Small models often produce verbose or incoherent outputs at these points,\\nwhile larger models handle them concisely. The proposed speculative thinking preserves\\nefficiency while leveraging the large model’s strength when most needed.\\na more accurate thinking step. The larger model then generates the next thought segment'),\n",
       " Document(metadata={}, page_content='efficiency while leveraging the large model’s strength when most needed.\\na more accurate thinking step. The larger model then generates the next thought segment\\nin place of the smaller model, effectively acting as a reasoning supervisor or corrector. This\\nlarge-model-aided intervention may enhance the robustness and accuracy of smaller models\\nby injecting stronger reasoning capabilities, thus balancing efficiency and performance.\\n3 Method: Speculative Thinking\\nWe propose a collaborative inference framework termed Speculative Thinking, where a\\nsmall model acts as speculative model and a large model serves as target model. Speculative\\nmodel performs primary reasoning, while target model intervenes selectively to provide\\nauxiliary thoughts when necessary. The overall framework is in Figure 4, . Target model\\ntakes over speculative model’s generation under the following three scenarios. The hyper-\\nparameters for Speculative Thinking—such as the selection of Reflection and Affirmation'),\n",
       " Document(metadata={}, page_content='takes over speculative model’s generation under the following three scenarios. The hyper-\\nparameters for Speculative Thinking—such as the selection of Reflection and Affirmation\\nkeywords, and the values of control parameters n1, n2, and n3 are shown in Appendix A.2.\\n(1) Affirmation/Reflection Takeover. This mechanism leverages stronger reasoning ability\\nof target model to help speculative model decide whether to continue or revise. Speculative\\nmodel first generates responses until a delimiter token (e.g., \\\\n\\\\n) is encountered. After this\\ndelimiter, speculative model generates one full sentence (i.e., n1 tokens). We then classify\\nthe sentence into three situations: Affirmation, Reflection, or Statement, based on keyword\\nmatching, as shown in Appendix A.2. If speculative model’s sentence is classified as either\\nAffirmation or Reflection, target model immediately takes over and generates n1 tokens.\\nSpeculative model then resumes generation conditioned on target model’s output.'),\n",
       " Document(metadata={}, page_content='Affirmation or Reflection, target model immediately takes over and generates n1 tokens.\\nSpeculative model then resumes generation conditioned on target model’s output.\\n(2) Verification Takeover. We observe that small models often struggle with effective\\nverification. To address this, we introduce a verification-triggered intervention. Whenever a\\n\\\\n\\\\n delimiter is encountered—regardless of whether the subsequent sentence is generated\\nby the speculative or target model—we examine if the sentence contains verification-related\\ncues (e.g., verify, double-check, etc.). If such cues are detected, target model takes over to\\ngenerate n2 tokens, assisting the verification process and mitigating false conclusions.\\n(3) Excessive Reflection Takeover. Our analysis reveals that a hallmark of incorrect answers\\nis excessive backtracking, where the model repeatedly negates its own thoughts. To mitigate\\nthis, we implement a negativity counter c that tracks the number of reflection sentences.'),\n",
       " Document(metadata={}, page_content='is excessive backtracking, where the model repeatedly negates its own thoughts. To mitigate\\nthis, we implement a negativity counter c that tracks the number of reflection sentences.\\nEach time a \\\\n\\\\nis encountered, we evaluate whether the following sentence is negative; if\\nso, we increment c. Once c exceeds a predefined threshold, we prompt the model to exit the\\nreflection loop. Specifically, we insert an auxiliary sentence (e.g., “Let us check whether there\\nare some wrong steps.”) into the output, and then delegate the next n3 tokens to target model.\\nThis mechanism serves to reorient speculative model and prevent reflection thinking loops.\\n5Preprint. Under review.\\nTable 2: Accuracy, average output length, and estimated speed of models on four datasets.\\nHere, 1.5B refers to the Deepseek-Distilled Qwen-2.5-1.5B model. “+” means with the help\\nof large models. modify ratio indicates the proportion of tokens in the final output that'),\n",
       " Document(metadata={}, page_content='Here, 1.5B refers to the Deepseek-Distilled Qwen-2.5-1.5B model. “+” means with the help\\nof large models. modify ratio indicates the proportion of tokens in the final output that\\ncome from the target model. After applying Speculative Thinking, both 1.5B and 7B models\\ndemonstrate improvements in accuracy, output length, and estimated inference speed. The\\nimprovement in estimated speed is measured relative to the corresponding target model.\\nDataset Speculative Target Modify Acc Length Estimated\\npass@1 Model Model Ratio (%) Improv. Avg Decr. Speed Improv.\\nAIME\\n1.5B\\n– – 25.6 – 17800.0 – 198.9 –\\n+14B 18.0% 33.3 +7.7 16691.2 -6.2% 110.3 +121.1%\\n+32B 19.0% 32.2 +6.6 15706.1 -11.7% 85.8 +185.9%\\n7B – – 48.9 – 13250.4 – 56.4 –\\n+32B 18.0% 53.3 +4.4 13213.6 -0.3% 41.0 +36.8%\\n14B – – 60.0 – 12600.2 – 49.9 –\\n32B – – 65.6 – 12274.3 – 30.0 –\\nGPQA\\n1.5B\\n– – 33.8 – 7922.0 – 223.2 –\\n+14B 15.0% 38.9 +5.1 8134.3 +2.7% 128.1 +121.7%\\n+32B 17.0% 41.9 +8.1 7612.4 -3.9% 91.8 +190.4%'),\n",
       " Document(metadata={}, page_content='14B – – 60.0 – 12600.2 – 49.9 –\\n32B – – 65.6 – 12274.3 – 30.0 –\\nGPQA\\n1.5B\\n– – 33.8 – 7922.0 – 223.2 –\\n+14B 15.0% 38.9 +5.1 8134.3 +2.7% 128.1 +121.7%\\n+32B 17.0% 41.9 +8.1 7612.4 -3.9% 91.8 +190.4%\\n7B – – 45.5 – 6111.5 – 62.1 –\\n+32B 22.0% 52.0 +6.5 5952.5 -2.6% 40.3 +27.5%\\n14B – – 57.1 – 5762.7 – 57.8 –\\n32B – – 61.6 – 5406.8 – 31.6 –\\nMATH500\\n1.5B\\n– – 83.2 – 5439.1 – 242.6 –\\n+14B 19.0% 89.0 +5.8 4527.4 -16.8% 134.6 +124.0%\\n+32B 19.0% 89.4 +6.2 4582.8 -15.7% 96.6 +200.0%\\n7B – – 92.8 – 3975.2 – 63.7 –\\n+32B 18.0% 93.0 +0.2 3767.8 -5.2% 46.0 +42.9%\\n14B – – 93.8 – 3609.0 – 60.1 –\\n32B – – 92.8 – 3802.2 – 32.2 –\\nAMC23\\n1.5B\\n– – 75.0 – 10460.8 – 212.7 –\\n+14B 19.0% 85.0 +10.0 7503.2 -28.3% 123.7 +123.0%\\n+32B 21.0% 80.0 +5.0 8691.2 -16.9% 82.8 +170.0%\\n7B – – 92.5 – 6093.8 – 62.6 –\\n+32B 16.0% 92.5 +0.0 5116.1 -16.1% 48.0 +56.4%\\n14B – – 95.0 – 6395.4 – 55.5 –\\n32B – – 95.0 – 7106.7 – 30.7 –\\n4 Experiments\\n4.1 Large Reasoning Models Monitor Small Reasoning Models'),\n",
       " Document(metadata={}, page_content='+32B 16.0% 92.5 +0.0 5116.1 -16.1% 48.0 +56.4%\\n14B – – 95.0 – 6395.4 – 55.5 –\\n32B – – 95.0 – 7106.7 – 30.7 –\\n4 Experiments\\n4.1 Large Reasoning Models Monitor Small Reasoning Models\\nThis experiment aims to evaluate the effectiveness of Speculative Thinking. We adopt three\\nkey evaluation metrics: accuracy, average output length, and estimated inference speed,\\nto fully assess the trade-off between reasoning performance and efficiency. The rationale\\nfor choosing the estimated inference speed, along with the details of its computation, is\\nprovided at the end of this section. We conduct experiments on four benchmark datasets:\\nAIME 2022–2024, GPQA-Diamond, MATH500, and AMC23.\\nAnalysis of results of Large Reasoning Models Monitor Small Reasoning Models. The re-\\nsults are summarized in Table 2, which demonstrates that our method consistently improves\\naccuracy while reducing unnecessary output length and enhancing inference speed. For ex-'),\n",
       " Document(metadata={}, page_content='sults are summarized in Table 2, which demonstrates that our method consistently improves\\naccuracy while reducing unnecessary output length and enhancing inference speed. For ex-\\nample, after being assisted by the 32B target model, the 1.5B speculative model demonstrates\\nconsistent and significant improvements across multiple datasets. Specifically, its accuracy\\nincreases by 6.2% on MATH500 , 8.1% on GPQA , 5.0% on AMC23 , and 6.6% on AIME . In\\naddition, the average output length is reduced by15.7%, 3.9%, 16.9% and 11.7% on the same\\ndatasets, respectively, indicating that the speculative model is able to reach conclusions\\nmore efficiently with guidance from the large model. Furthermore, in terms of estimated\\n6Preprint. Under review.\\ngeneration speed, the 1.5B model assisted by the 32B model consistently outperforms the\\nstandalone 32B model, despite leveraging it selectively. These findings collectively demon-'),\n",
       " Document(metadata={}, page_content='generation speed, the 1.5B model assisted by the 32B model consistently outperforms the\\nstandalone 32B model, despite leveraging it selectively. These findings collectively demon-\\nstrate the effectiveness and practicality of our Speculative Thinking framework, offering a\\npromising trade-off between performance and computational efficiency. Moreover, when\\nassisting the smaller reasoning model, the target model only needs to modify approximately\\n20% of the speculative model’s output to significantly enhance its reasoning performance.\\nFigure 5: A comparison between the prefix\\nand decode stages reveals that the time (in\\nseconds) required to process multiple tokens\\nduring the prefix phase is nearly equivalent\\nto the time taken to decode a single token.\\nModel decode prefix\\nn=1 n=1 n=20 n=250\\n1.5B 0.036 0.036 0.040 0.045\\n32B 0.09 0.11 0.12 0.15\\nTheoretical Estimation of FLOPs and To-\\nken Generation Speed. We adopt a theo-\\nretical analysis rather than empirical tim-'),\n",
       " Document(metadata={}, page_content='n=1 n=1 n=20 n=250\\n1.5B 0.036 0.036 0.040 0.045\\n32B 0.09 0.11 0.12 0.15\\nTheoretical Estimation of FLOPs and To-\\nken Generation Speed. We adopt a theo-\\nretical analysis rather than empirical tim-\\ning, since our method— Speculative Think-\\ning—primarily introduces logical coordina-\\ntion between models. In contrast, runtime\\nmeasurements would be significantly af-\\nfected by backend GPU optimizations, es-\\npecially in systems like vLLM (Kwon et al.,\\n2023). The computation of FLOPs for prefill\\nand decode stages is in Appendix A.1. The\\ndifferences between prefix and decode are shown in Figure 5.\\nWe empirically profile average inference time for both decode and prefix stages across\\nvarious model sizes and output token lengths. These measurements are obtained using\\ngenerate() api from HuggingFace Transformers, with key-value cache enabled for the\\nprompt. We observe that when GPU memory are sufficient, the average time in prefix stage'),\n",
       " Document(metadata={}, page_content='generate() api from HuggingFace Transformers, with key-value cache enabled for the\\nprompt. We observe that when GPU memory are sufficient, the average time in prefix stage\\nremains relatively stable across positions. We could see time required to process multiple\\ntokens during the prefix phase is nearly equivalent to the time taken to decode a single\\ntoken. To reflect the difference, we assume a speedup for the prefix stage :FLOPsprefix(m) =\\nFLOPsdecode(n = 1), where m and n mean the token number. We set GPU computational\\ncapacity to 3.12 × 1010 FLOPs/s, which corresponds to a A100-class GPU. The estimated\\nspeed is calculated as follows:\\nEstimated Speed = Total Tokens\\x10\\nFLOPsprefill + FLOPsprefix + FLOPsdecode\\n\\x11\\n/GPU Capacity\\n(1)\\n4.2 Reasoning Models Monitor Non-Reasoning Models\\nGiven that large reasoning models can effectively assist smaller reasoning models, a natural\\nfollow-up question is: Can we leverage reasoning-capable models to enhance the performance'),\n",
       " Document(metadata={}, page_content='Given that large reasoning models can effectively assist smaller reasoning models, a natural\\nfollow-up question is: Can we leverage reasoning-capable models to enhance the performance\\nand accuracy of non-reasoning models ? To explore this, we adapt the Speculative Thinking\\nframework to monitor a speculative model that lacks inherent reasoning capability.\\nModification for speculative thinking applied to non-reasoning models. Specifically, in\\nAffirmation/Reflection Takeover, we originally determine whether the speculative model’s\\nsentence following a \"\\\\n\\\\n\" contains reflective or Affirmative reasoning cues. However,\\nnon-reasoning models typically do not emit such linguistic signals. Therefore, in this setting,\\nwe directly allow target model to take over and generate the next sentence after each\\n\"\\\\n\\\\n\". In addition, we further enhance the speculative model by allowing target model to\\ngenerate the first 100 tokens before any question answering begins. This is motivated by the'),\n",
       " Document(metadata={}, page_content='\"\\\\n\\\\n\". In addition, we further enhance the speculative model by allowing target model to\\ngenerate the first 100 tokens before any question answering begins. This is motivated by the\\nobservation that reasoning models often preface their answers with structured setups such\\nas “Okay, so I have this problem where I need...”, which helps guide the generation for models.\\nAnalysis of Results of Reasoning Models Monitor Non-Reasoning Models. The results,\\nwhere a non-reasoning model is augmented by a reasoning-capable target model, are shown\\nin Table 3. We first observe that Qwen-2.5-7B-Instruct, a non-reasoning model, benefits\\nnotably from speculative assistance by both 7B and 32B reasoning models. For instance,\\non the MATH500 dataset, its accuracy improves from 74.0% to 81.8%. However, this\\nimprovement comes at the cost of increased output length, indicating a trade-off between\\nenhanced reasoning ability and generation efficiency. However, when assisted by the 1.5B'),\n",
       " Document(metadata={}, page_content='improvement comes at the cost of increased output length, indicating a trade-off between\\nenhanced reasoning ability and generation efficiency. However, when assisted by the 1.5B\\nreasoning model, performance improvements are not consistently observed. This indicates\\n7Preprint. Under review.\\nTable 3: Accuracy, average output length, and estimated speed on four datasets. 7B-Instruct\\nrefers to Qwen-2.5-7B-Instruct. “+” means with the help of reasoning models. Modify\\nratio indicates the proportion of tokens in the final output that come from target model.\\nAfter applying Speculative Thinking, models demonstrate improvements in accuracy. The\\nimprovement in estimated speed is measured relative to the corresponding target model.\\nDataset Speculative Target Avg Modify Estimated Acc\\npass@1 Model Model Length Ratio Speed (%) Improv.\\nAIME 7B-Instruct\\n– 1249.8 – 64.7 7.8 –\\n+1.5B 8029.3 54.0% 51.5 6.7 -1.1\\n+7B 10458.5 42.0% 38.8 13.3 +5.5\\n+32B 10236.0 46.0% 29.0 15.6 +7.8\\nGPQA 7B-Instruct'),\n",
       " Document(metadata={}, page_content='AIME 7B-Instruct\\n– 1249.8 – 64.7 7.8 –\\n+1.5B 8029.3 54.0% 51.5 6.7 -1.1\\n+7B 10458.5 42.0% 38.8 13.3 +5.5\\n+32B 10236.0 46.0% 29.0 15.6 +7.8\\nGPQA 7B-Instruct\\n– 5.6 – 1.5 33.8 –\\n+1.5B 6763.8 43.0% 45.6 31.8 -2.0\\n+7B 4739.7 42.0% 36.8 40.9 +7.1\\n+32B 6652.8 31.0% 33.6 48.0 +14.2\\nMATH500 7B-Instruct\\n– 802.3 – 58.3 74.0 –\\n+1.5B 3368.8 43.0% 53.1 74.8 +0.8\\n+7B 3172.0 44.0% 41.2 79.2 +5.2\\n+32B 3015.9 44.0% 31.7 81.8 +7.8\\nAMC23 7B-Instruct\\n– 878.5 – 64.8 42.5 –\\n+1.5B 7603.0 49.0% 48.4 55.0 +12.5\\n+7B 6431.5 43.0% 39.0 67.5 +25.0\\n+32B 8732.8 31.0% 33.5 55.0 +12.5\\nthat, during the design of speculative thinking systems, it is preferable to choose a target\\nmodel that is either of equal size or larger than the speculative model, and more importantly,\\npossesses stronger reasoning capabilities. Mismatches where the speculative model is larger\\nor stronger than the target model may lead to suboptimal or even detrimental outcomes.\\n4.3 Comparisons between Speculative Decoding and Speculative Thinking'),\n",
       " Document(metadata={}, page_content='or stronger than the target model may lead to suboptimal or even detrimental outcomes.\\n4.3 Comparisons between Speculative Decoding and Speculative Thinking\\nMATH500 AIME\\n60\\n80\\n100Accuracy (%)\\n92.8\\n65.6\\n93.0\\n58.9\\n93.0\\n53.3\\n32B\\nDecoding\\nThinking\\nMATH500 AIME\\n25\\n30\\n35\\n40\\n45\\n50Speed (token/s)\\n30.0\\n32.230.4 28.7\\n46.0\\n41.0\\nFigure 6: Comparison between Speculative Decoding and Thinking using a 7B speculative\\nmodel and a 32B target model. In Speculative Decoding, speculative model generates 20\\ntokens per step to match the number of intervention tokens in Speculative Thinking.\\nThis experiment primarily compares the differences between speculative decoding and\\nspeculative thinking. Due to the constraint that speculative decoding requires the specula-\\ntive model and the target model to have the same vocabulary size, we obtain speculative\\ndecoding results where the speculative model is 7B, and the target model is 32B. To align'),\n",
       " Document(metadata={}, page_content='tive model and the target model to have the same vocabulary size, we obtain speculative\\ndecoding results where the speculative model is 7B, and the target model is 32B. To align\\nwith Speculative Thinking, which takes over the generation of 20 tokens at a time, we set\\nthe speculative model in speculative decoding to generate n = 20 tokens per step.\\nSpeculative decoding relies on the speculative and target models having similar token\\noutput distributions to accelerate generation. In contrast, Speculative Thinking focuses on\\nenhancing the speculative model’s reasoning with lightweight assistance from target model,\\nwithout strictly requiring token distributional alignment. As shown in in Figure 6, although\\nspeculative decoding matchs the accuracy of 32B model, it often suffers from a high rejection\\n8Preprint. Under review.\\nrate—nearly 50% of tokens need to be regenerated by target model, which diminishes its'),\n",
       " Document(metadata={}, page_content='8Preprint. Under review.\\nrate—nearly 50% of tokens need to be regenerated by target model, which diminishes its\\nspeed. Speculative Thinking avoids this issue by allowing the target model to intervene\\nonly when necessary, improving the speculative model’s reasoning with minimal overhead.\\n5 Related Works\\nLLM Reasoning. Current approaches to enhancing the reasoning capabilities (Chen et al.,\\n2025a; Plaat et al., 2024; Sun et al., 2023) of language models primarily fall into two categories:\\nreinforcement learning (Schulman et al., 2017) and supervised fine-tuning (Jaech et al., 2024;\\nYang et al., 2024). For instance, DeepSeek (Guo et al., 2025; Liu et al., 2024) achieved\\nstate-of-the-art reasoning performance using GRPO (Shao et al., 2024; Yu et al., 2025), and\\nfurther improved smaller models by distilling high-quality reasoning traces. This line of\\nresearch has inspired numerous efforts to replicate DeepSeek-R1 with the goal of uncovering'),\n",
       " Document(metadata={}, page_content='further improved smaller models by distilling high-quality reasoning traces. This line of\\nresearch has inspired numerous efforts to replicate DeepSeek-R1 with the goal of uncovering\\npotential “aha moments” in reasoning, including works such as Logic RL (Xie et al., 2025)\\nand SimpleRL-Zoo (Zeng et al., 2025). Many studies also use SFT to improve reasoning,\\nincluding SkyThought-T1 (Team, 2025b) and Bespoke-Stratos-32B (Labs, 2025), which collect\\nand fine-tune on carefully curated high-quality reasoning data. Several works have further\\ninvestigated key techniques for enhancing reasoning performance during RL (Baek &\\nTegmark, 2025; Yeo et al., 2025) or SFT (Chen et al., 2025b; 2024a; Tian et al., 2025; Liu et al.,\\n2025b). For example, (Li et al., 2025a) argues that the structure of reasoning steps in the\\ndata is more critical than the actual content; (Ji et al., 2025) highlights the importance of the'),\n",
       " Document(metadata={}, page_content='2025b). For example, (Li et al., 2025a) argues that the structure of reasoning steps in the\\ndata is more critical than the actual content; (Ji et al., 2025) highlights the importance of the\\ninitial few tokens in each reasoning instance for optimizing model performance. In addition,\\nseveral recent studies—such as s1(Muennighoff et al., 2025) emphasize the value of selecting\\na small set of high-quality reasoning samples to drive efficient model improvement.\\nEfficient Reasoning. Current reasoning models still exhibit notable limitations (Bandyopad-\\nhyay et al., 2025; Li et al., 2025c). One prominent issue is excessive response length—many\\nreasoning-enabled models tend to generate unnecessarily verbose outputs. As a result, effi-\\ncient reasoning has become an emerging research focus. An early effort in this direction was\\nproposed by Kimi 1.5 (Team et al., 2025), which introduced the Long-to-Short method. This'),\n",
       " Document(metadata={}, page_content='cient reasoning has become an emerging research focus. An early effort in this direction was\\nproposed by Kimi 1.5 (Team et al., 2025), which introduced the Long-to-Short method. This\\napproach collects paired long and short responses and applies Direct Preference Optimiza-\\ntion (Rafailov et al., 2023; Zeng et al., 2024) to train models that prefer concise answers. The\\nidea was later reproduced by Sky-Thought (Team, 2025a), further validating its effectiveness.\\nTokenSkip (Xia et al., 2025), which improves efficiency by identifying and removing redun-\\ndant or uninformative tokens to create cleaner training data. LightThinker (Zhang et al.,\\n2025) takes a different route by explicitly compressing intermediate thoughts to generate\\nshorter yet informative reasoning traces, thereby enabling models to produce more concise\\noutputs via fine-tuning. Wang et al. (2025); Sui et al. (2025a) highlights a counterintuitive'),\n",
       " Document(metadata={}, page_content='shorter yet informative reasoning traces, thereby enabling models to produce more concise\\noutputs via fine-tuning. Wang et al. (2025); Sui et al. (2025a) highlights a counterintuitive\\nphenomenon: when reasoning fails, model outputs often become significantly longer. This is\\nattributed to repetitive generation of reasoning-supportive tokens like “wait”, which reflect\\nthe model’s tendency to over-compensate by generating more thoughts. Other notable\\napproaches include Dynasor(Fu et al., 2024), which uses probing techniques to detect and\\nterminate reasoning early. There are some other works including efficient reaosning (Aytes\\net al., 2025; Lee et al., 2025; Sui et al., 2025c; Xu et al., 2025; Liao et al., 2025).\\n6 Conclusion\\nWe propose Speculative Thinking, a training-free framework that leverages larger reasoning\\nmodels to guide smaller ones through selective delegation at structurally meaningful points'),\n",
       " Document(metadata={}, page_content='6 Conclusion\\nWe propose Speculative Thinking, a training-free framework that leverages larger reasoning\\nmodels to guide smaller ones through selective delegation at structurally meaningful points\\nin generation. By exploiting the natural reasoning patterns of LLMs—particularly reflec-\\ntion cues like \"\\\\n\\\\n\"—our approach significantly enhances both accuracy, average output\\nlength and efficiency without any additional training in four math reasoning datasets like\\nMATH500. Experiments demonstrate substantial gains in performance and output con-\\nciseness, underscoring the potential of collaborative inference between models of different\\ncapacities. This highlights a promising paradigm for improving reasoning of reasoning and\\nnon-reasoning models without additional data or training computation cost.\\n9Preprint. Under review.\\nLimitations\\nSpeculative Thinking relies on the assistance of a larger target model to improve the reason-'),\n",
       " Document(metadata={}, page_content='9Preprint. Under review.\\nLimitations\\nSpeculative Thinking relies on the assistance of a larger target model to improve the reason-\\ning ability and reduce the output length of a smaller speculative model. For this framework\\nto be effective, target model must possess stronger reasoning capabilities than speculative\\nmodel. Additionally, our current implementation assumes that both models belong to the\\nsame model family, which allows us to leverage shared KV cache structures to accelerate\\ninference. Finally, we observe that the performance of Speculative Thinking is sensitive\\nto prompt quality—utilizing an optimized prompt for each model is critical to achieving\\nthe best results, like “Please reason step by step, and put your final answer within\\n\\\\boxed{}.”.\\nReferences\\nSimon A Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reason-\\ning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025.'),\n",
       " Document(metadata={}, page_content='\\\\boxed{}.”.\\nReferences\\nSimon A Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reason-\\ning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025.\\nDavid D. Baek and Max Tegmark. Towards understanding distilled reasoning models: A\\nrepresentational approach, 2025. URL https://arxiv.org/abs/2503.03730.\\nDibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal. Thinking machines: A\\nsurvey of llm based reasoning strategies. arXiv preprint arXiv:2503.10814, 2025.\\nQiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking\\nthe capabilities of thought: A reasoning boundary framework to quantify and optimize\\nchain-of-thought. Advances in Neural Information Processing Systems, 37:54872–54904, 2024a.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang\\nHu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: A survey of long'),\n",
       " Document(metadata={}, page_content='Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang\\nHu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: A survey of long\\nchain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567,\\n2025a.\\nXinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui\\nSu, Yijie Pan, Dietrich Klakow, Wenjie Li, et al. Unveiling the key factors for distilling\\nchain-of-thought reasoning. arXiv preprint arXiv:2502.18001, 2025b.\\nYushuo Chen, Tianyi Tang, Erge Xiang, Linjiang Li, Wayne Xin Zhao, Jing Wang, Yunpeng\\nChai, and Ji-Rong Wen. Towards coarse-to-fine evaluation of inference efficiency for large\\nlanguage models. arXiv preprint arXiv:2404.11502, 2024b.\\nLi Chenglin, Qianglong Chen, Liangyue Li, Caiyu Wang, Feng Tao, Yicheng Li, Zulong\\nChen, and Yin Zhang. Mixed distillation helps smaller language models reason better. In'),\n",
       " Document(metadata={}, page_content='Li Chenglin, Qianglong Chen, Liangyue Li, Caiyu Wang, Feng Tao, Yicheng Li, Zulong\\nChen, and Yin Zhang. Mixed distillation helps smaller language models reason better. In\\nFindings of the Association for Computational Linguistics: EMNLP 2024, pp. 1673–1690, 2024.\\nYichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, and\\nHao Zhang. Efficiently serving llm reasoning programs with certaindex. arXiv preprint\\narXiv:2412.20993, 2024.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\\nllms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\\nXiaotian Han. Reproduce the inference time scaling exp, 2024. URL https://ahxt.github.\\nio/blog/2024-12-30-inference-time-scaling-exp/ . 2024-12-30.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low,'),\n",
       " Document(metadata={}, page_content='io/blog/2024-12-30-inference-time-scaling-exp/ . 2024-12-30.\\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low,\\nAlec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card.\\narXiv preprint arXiv:2412.16720, 2024.\\nKe Ji, Jiahao Xu, Tian Liang, Qiuzhi Liu, Zhiwei He, Xingyu Chen, Xiaoyuan Liu, Zhijie\\nWang, Junying Chen, Benyou Wang, et al. The first few tokens are all you need: An\\nefficient and effective unsupervised prefix fine-tuning method for reasoning models.arXiv\\npreprint arXiv:2503.02875, 2025.\\n10Preprint. Under review.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\\nlanguage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th\\nSymposium on Operating Systems Principles, 2023.\\nBespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distil-'),\n",
       " Document(metadata={}, page_content='Symposium on Operating Systems Principles, 2023.\\nBespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distil-\\nlation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-\\nreasoning-distillation, 2025. Accessed: 2025-01-22.\\nAyeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-\\nthought? a token complexity approach. arXiv preprint arXiv:2503.01141, 2025.\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via\\nspeculative decoding. In International Conference on Machine Learning, pp. 19274–19286.\\nPMLR, 2023.\\nDacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde,\\nKourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion\\nStoica. Llms can easily learn to reason from demonstrations structure, not content, is what\\nmatters!, 2025a. URL https://arxiv.org/abs/2502.07374.'),\n",
       " Document(metadata={}, page_content='Stoica. Llms can easily learn to reason from demonstrations structure, not content, is what\\nmatters!, 2025a. URL https://arxiv.org/abs/2502.07374.\\nYuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar\\nRamasubramanian, and Radha Poovendran. Small models struggle to learn from strong\\nreasoners. arXiv preprint arXiv:2502.12143, 2025b.\\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao,\\nHaotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A\\nsurvey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025c.\\nBaohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sa-\\nhoo, and Caiming Xiong. Reward-guided speculative decoding for efficient llm reasoning.\\narXiv preprint arXiv:2501.19324, 2025.\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,'),\n",
       " Document(metadata={}, page_content='arXiv preprint arXiv:2501.19324, 2025.\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,\\nJan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv\\npreprint arXiv:2305.20050, 2023.\\nAixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr,\\nChong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: A strong, economical, and\\nefficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024.\\nRunze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and\\nBowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling,\\n2025a. URL https://arxiv.org/abs/2502.06703.\\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee,\\nand Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint\\narXiv:2503.20783, 2025b.\\nZhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D.'),\n",
       " Document(metadata={}, page_content='and Min Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint\\narXiv:2503.20783, 2025b.\\nZhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D.\\nLane, and Mengwei Xu. Small language models: Survey, measurements, and insights,\\n2025. URL https://arxiv.org/abs/2409.15790.\\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi,\\nLuke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple\\ntest-time scaling, 2025. URL https://arxiv.org/abs/2501.19393.\\nChien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian\\nChen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, Junda Wu, Ashish Singh, Yu Wang,\\nJiuxiang Gu, Franck Dernoncourt, Nesreen K. Ahmed, Nedim Lipka, Ruiyi Zhang, Xiang\\nChen, Tong Yu, Sungchul Kim, Hanieh Deilamsalehy, Namyong Park, Mike Rimer, Zhehao\\nZhang, Huanrui Yang, Ryan A. Rossi, and Thien Huu Nguyen. A survey of small language'),\n",
       " Document(metadata={}, page_content='Chen, Tong Yu, Sungchul Kim, Hanieh Deilamsalehy, Namyong Park, Mike Rimer, Zhehao\\nZhang, Huanrui Yang, Ryan A. Rossi, and Thien Huu Nguyen. A survey of small language\\nmodels, 2024. URL https://arxiv.org/abs/2410.20011.\\n11Preprint. Under review.\\nAske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back.\\nReasoning with large language models, a survey. arXiv preprint arXiv:2407.11511, 2024.\\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\\nmodel. Advances in Neural Information Processing Systems, 36:53728–53741, 2023.\\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang,\\nJulien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-\\nproof q&a benchmark. In First Conference on Language Modeling, 2024.'),\n",
       " Document(metadata={}, page_content='Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-\\nproof q&a benchmark. In First Conference on Language Modeling, 2024.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\\nMingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\\nreasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\\nGaurav Srivastava, Shuxiang Cao, and Xuan Wang. Towards reasoning ability of small\\nlanguage models. arXiv preprint arXiv:2502.11569, 2025.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient\\nreasoning for large language models. arXiv preprint arXiv:2503.16419, 2025a.'),\n",
       " Document(metadata={}, page_content='Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: A survey on efficient\\nreasoning for large language models. arXiv preprint arXiv:2503.16419, 2025a.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A\\nsurvey on efficient reasoning for large language models, 2025b. URL https://arxiv.org/\\nabs/2503.16419.\\nYuan Sui, Yufei He, Tri Cao, Simeng Han, and Bryan Hooi. Meta-reasoner: Dynamic\\nguidance for optimized inference-time reasoning in large language models. arXiv preprint\\narXiv:2502.19918, 2025c.\\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi\\nXu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey of reasoning with\\nfoundation models. arXiv preprint arXiv:2312.11562, 2023.\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,'),\n",
       " Document(metadata={}, page_content='foundation models. arXiv preprint arXiv:2312.11562, 2023.\\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,\\nChenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement\\nlearning with llms. arXiv preprint arXiv:2501.12599, 2025.\\nNovaSky Team. Think less, achieve more: Cut reasoning costs by 50 https://novasky-\\nai.github.io/posts/reduce-overthinking, 2025a. Accessed: 2025-01-23.\\nNovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novasky-\\nai.github.io/posts/sky-t1, 2025b. Accessed: 2025-01-09.\\nXiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yunjie Ji, Yiping Peng, Han Zhao,\\nand Xiangang Li. Think twice: Enhancing llm reasoning by scaling multi-round test-time\\nthinking, 2025. URL https://arxiv.org/abs/2503.19855.\\nYue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song,\\nDian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong'),\n",
       " Document(metadata={}, page_content='Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song,\\nDian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong\\nYu. Thoughts are all over the place: On the underthinking of o1-like llms, 2025. URL\\nhttps://arxiv.org/abs/2501.18585.\\nHeming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Con-\\ntrollable chain-of-thought compression in llms, 2025. URL https://arxiv.org/abs/2502.\\n12067.\\nTian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai\\nQiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based\\nreinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768.\\n12Preprint. Under review.\\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by\\nwriting less. arXiv preprint arXiv:2502.18600, 2025.\\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,'),\n",
       " Document(metadata={}, page_content='writing less. arXiv preprint arXiv:2502.18600, 2025.\\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\\nDayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint\\narXiv:2412.15115, 2024.\\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is\\nmore for reasoning, 2025. URL https://arxiv.org/abs/2502.03387.\\nEdward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long\\nchain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan,\\nGaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement\\nlearning system at scale. arXiv preprint arXiv:2503.14476, 2025.\\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He.\\nSimplerl-zoo: Investigating and taming zero reinforcement learning for open base models'),\n",
       " Document(metadata={}, page_content='Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He.\\nSimplerl-zoo: Investigating and taming zero reinforcement learning for open base models\\nin the wild, 2025. URL https://arxiv.org/abs/2503.18892.\\nYongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang.\\nToken-level direct preference optimization. arXiv preprint arXiv:2404.11999, 2024.\\nJintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng,\\nHuajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression.\\narXiv preprint arXiv:2502.15589, 2025.\\nYunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae\\nLee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-\\ncorrect reasoning. arXiv preprint arXiv:2404.17140, 2024.\\n13Preprint. Under review.\\nA Appendix\\nA.1 Compuation of FLOPs\\nFLOPsprefill(s) =8sh2 + 16sh + 4s2h + 4s2n + 6shh′ + 2sh′ (2)\\nFLOPsdecode(s) =8h2 + 16h + 4sh + 4sn + 6hh′ + 2h′ (3)'),\n",
       " Document(metadata={}, page_content='13Preprint. Under review.\\nA Appendix\\nA.1 Compuation of FLOPs\\nFLOPsprefill(s) =8sh2 + 16sh + 4s2h + 4s2n + 6shh′ + 2sh′ (2)\\nFLOPsdecode(s) =8h2 + 16h + 4sh + 4sn + 6hh′ + 2h′ (3)\\nFLOPstotal = FLOPsprefill(pl) +\\ndl −1\\n∑\\ni=0\\nFLOPsdecode(pl + i) (4)\\nWe compute the FLOPs of prefill and decoding stages based on Chen et al. (2024b); Han\\n(2024), where the batch size is 1. s is the input sequence length. h is the hidden size. h′ is the\\nintermediate size of the feed-forward network (FFN). n is the number of attention heads. d\\nis the size of each attention head, such that h = nd. pl is the length of the problem prompt.\\ndl is the number of tokens to be generated in the solution.\\n�1, �1 �2, �2\\n�3\\n�1, �1 �2, �2\\n�1, �1 �2, �2 �3, �3\\nDecode Prefix\\n�3\\n�4 �4\\nkv cache\\n�1, �1\\n�2, �2\\n�3\\n �4\\n�1, �1\\n�2, �2\\n�3\\n�1, �1\\n�2, �2\\n�3, �3\\n�4\\nDecode\\nPrefix\\n   kv \\ncache\\n(a) decode v.s. prefix\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n2\\n4\\n6\\n8Time (seconds)\\ndecode\\nprefix (b) Deepseek-1.5B\\n0 50 100 150 200 250'),\n",
       " Document(metadata={}, page_content='�3\\n�1, �1\\n�2, �2\\n�3, �3\\n�4\\nDecode\\nPrefix\\n   kv \\ncache\\n(a) decode v.s. prefix\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n2\\n4\\n6\\n8Time (seconds)\\ndecode\\nprefix (b) Deepseek-1.5B\\n0 50 100 150 200 250\\nGenerated T oken Num\\n0\\n5\\n10\\n15\\n20Time (seconds)\\ndecode\\nprefix (c) Deepseek-32B\\nFigure 7: Comparison between Decode and Prefix stages: average time consumed by the\\n1.5B and 32B models when generating different numbers of output tokens. As the number\\nincreases, decoding time grows significantly, while prefix time remains nearly constant.\\nA.2 Hyperparameters of Speculative Thinking\\nA sentence is labeled Affirmation or Reflection if it contains affirmation cues (e.g., yes, yep)\\nor backtracking cues (e.g., wait, alternatively); and Statement if neither type is present. If\\nboth Affirmation and Reflection keywords appear, the decision is made based on majority\\ncount, and in case of a tie, we default to Reflection.'),\n",
       " Document(metadata={}, page_content='both Affirmation and Reflection keywords appear, the decision is made based on majority\\ncount, and in case of a tie, we default to Reflection.\\nWithin the proposed framework, we define three sets of indicative keywords that trigger\\ndifferent forms of target model intervention:\\n• Reflection keywords, used to detect reflection or hesitation: “wait”, “alternatively”,\\n“hold on”, “another”, “verify”, “think again”, “recap”, “check”.\\n• Affirmation keywords, indicating confidence or commitment to a line of reasoning:\\n“yeah”, “yes”, “final answer”, “confident”.\\n• Verification keywords, used to trigger verification-based intervention: “verify”,\\n“think again”, “recap”, “check”.\\nWe also configure fixed token lengths for the target model’s interventions in different\\nscenarios: n1 = 20 for Affirmation/Reflection Takeover,n2 = 125 for Verification Takeover,\\nand n3 = 125 for Excessive Negativity Takeover. These hyperparameters are selected to\\nbalance informativeness and computational cost.'),\n",
       " Document(metadata={}, page_content='and n3 = 125 for Excessive Negativity Takeover. These hyperparameters are selected to\\nbalance informativeness and computational cost.\\nA.3 Results of Deepseek-Distilled Qwen-2.5-7B\\nWe present the accuracy and average output length of Deepseek-Distilled Qwen-2.5-7B on\\nfour datasets.\\n14Preprint. Under review.\\n7B 7B+32B 32B\\n20\\n40\\n60Accuracy\\n48.89 53.33\\n65.56\\n7B 7B+32B 32B\\n11000\\n12000\\n13000\\n14000Average Length\\n13250 13214\\n12274\\n(a) AIME\\n7B 7B+32B 32B\\n90\\n91\\n92\\n93\\n94Accuracy\\n92.80 93.00 92.80\\n7B 7B+32B 32B\\n3600\\n3800\\n4000Average Length\\n3975\\n3768 3802 (b) MATH500\\n7B 7B+32B 32B\\n30\\n40\\n50\\n60Accuracy\\n45.45\\n52.02\\n61.62\\n7B 7B+32B 32B\\n5000\\n5500\\n6000Average Length\\n6111\\n5952\\n5407\\n(c) GPQA\\n7B 7B+32B 32B\\n90\\n92\\n94\\n96Accuracy\\n92.50 92.50\\n95.00\\n7B 7B+32B 32B\\n5000\\n6000\\n7000Average Length\\n6094\\n5116\\n7107 (d) AMC23\\nFigure 8: Accuracy and average output length of models on four datasets (AIME 2020–2024,\\nMATH500, GPQA, and AMC23). 1B denotes Deepseek-Distilled Qwen 2.5-7B model, 32B'),\n",
       " Document(metadata={}, page_content='6094\\n5116\\n7107 (d) AMC23\\nFigure 8: Accuracy and average output length of models on four datasets (AIME 2020–2024,\\nMATH500, GPQA, and AMC23). 1B denotes Deepseek-Distilled Qwen 2.5-7B model, 32B\\nrefers to Deepseek-Distilled Qwen 2.5-32B model, and 7B+32B represents Speculative Think-\\ning, where 32B model assists 7B model. Speculative Thinking leads to a significant improve-\\nment in the 7B model’s accuracy while effectively reducing its output length.\\nA.4 Proportion of Top-10 Preceding Tokens\\nTable 4: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in the\\nMATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-1.5B model.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.708) \" \" (0.207) \" \"(0.055) \").\\\\n\\\\n\"(0.011) \"?\\\\n\\\\n\"(0.008)\\n\" \\\\n\\\\n\"(0.004) \"\\\\n\\\\n\"(0.003) \")\\\\n\\\\n\"(0.001) \":\\\\n\\\\n\"(0.001) \" )\\\\n\\\\n\"(0.001)\\nhmm \" \" (0.689) \".\\\\n\\\\n\"(0.139) \")\\\\n\\\\n\"(0.043) \"]\\\\n\\\\n\"(0.037) \"\\\\n\\\\n\"(0.033)'),\n",
       " Document(metadata={}, page_content='\" \\\\n\\\\n\"(0.004) \"\\\\n\\\\n\"(0.003) \")\\\\n\\\\n\"(0.001) \":\\\\n\\\\n\"(0.001) \" )\\\\n\\\\n\"(0.001)\\nhmm \" \" (0.689) \".\\\\n\\\\n\"(0.139) \")\\\\n\\\\n\"(0.043) \"]\\\\n\\\\n\"(0.037) \"\\\\n\\\\n\"(0.033)\\n\").\\\\n\\\\n\"(0.027) \" \" (0.007) \"]\\\\n\"(0.007) \"?\\\\n\\\\n\"(0.004) \" \\\\n\\\\n\"(0.004)\\nwait \".\\\\n\\\\n\" (0.647) \" \"(0.230) \"?\\\\n\\\\n\"(0.044) \").\\\\n\\\\n\"(0.026) \"\\\\n\\\\n\"(0.016)\\n\")\\\\n\\\\n\"(0.009) \"]\\\\n\\\\n\"(0.007) \" \\\\n\\\\n\"(0.005) \" \" (0.004) \":\\\\n\\\\n\"(0.002)\\nTable 5: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in the\\nMATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-7B model.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.929) \" \"(0.048) \"?\\\\n\\\\n\"(0.008) \").\\\\n\\\\n\"(0.007) \" \\\\n\\\\n\"(0.004)\\n\")\\\\n\\\\n\"(0.001) \"?)\\\\n\\\\n\"(0.001) \"].\\\\n\\\\n\"(0.000) \"]\\\\n\\\\n\"(0.000) \"’.\\\\n\\\\n\"(0.000)\\nhmm \" \" (0.697) \".\\\\n\\\\n\"(0.123) \"\\\\n\\\\n\"(0.047) \")\\\\n\\\\n\"(0.043) \"]\\\\n\\\\n\"(0.038)\\n\").\\\\n\\\\n\"(0.025) \"?\\\\n\\\\n\"(0.006) \" \\\\n\\\\n\"(0.005) \"]\\\\n\"(0.003) \" )\\\\n\\\\n\"(0.003)'),\n",
       " Document(metadata={}, page_content='hmm \" \" (0.697) \".\\\\n\\\\n\"(0.123) \"\\\\n\\\\n\"(0.047) \")\\\\n\\\\n\"(0.043) \"]\\\\n\\\\n\"(0.038)\\n\").\\\\n\\\\n\"(0.025) \"?\\\\n\\\\n\"(0.006) \" \\\\n\\\\n\"(0.005) \"]\\\\n\"(0.003) \" )\\\\n\\\\n\"(0.003)\\nwait \".\\\\n\\\\n\" (0.637) \" \"(0.224) \"?\\\\n\\\\n\"(0.048) \").\\\\n\\\\n\"(0.029) \"\\\\n\\\\n\"(0.019)\\n\")\\\\n\\\\n\"(0.015) \" \\\\n\\\\n\"(0.007) \"]\\\\n\\\\n\"(0.005) \":\\\\n\\\\n\"(0.004) \" )\\\\n\\\\n\"(0.002)\\n15Preprint. Under review.\\nTable 6: Proportion of top-10 preceding tokens of reason-supportive words (like wait) in the\\nMATH500 dataset, as generated by the Deepseek-Distilled Qwen-2.5-14B model.\\nWord Top 10 frequent tokens before reasoning-supportive tokens (with probability)\\nalternatively \"\\\\n\\\\n\"(0.867) \" \"(0.076) \").\\\\n\\\\n\"(0.022) \"?\\\\n\\\\n\"(0.015) \" \\\\n\\\\n\"(0.013)\\n\")\\\\n\\\\n\"(0.001) \"\\\\n\\\\n\"(0.001) \"]\\\\n\\\\n\"(0.001) \"].\\\\n\\\\n\"(0.001) \" \" (0.001)\\nhmm \" \" (0.649) \".\\\\n\\\\n\"(0.159) \"\\\\n\\\\n\"(0.047) \")\\\\n\\\\n\"(0.036) \"]\\\\n\\\\n\"(0.033)\\n\").\\\\n\\\\n\"(0.033) \" \\\\n\\\\n\"(0.010) \"?\\\\n\\\\n\"(0.009) \"]\\\\n\"(0.007) }\\\\n\\\\n(0.004)\\nwait \".\\\\n\\\\n\" (0.643) \" \"(0.206) \"?\\\\n\\\\n\"(0.053) \").\\\\n\\\\n\"(0.032) \"\\\\n\\\\n\"(0.021)'),\n",
       " Document(metadata={}, page_content='\").\\\\n\\\\n\"(0.033) \" \\\\n\\\\n\"(0.010) \"?\\\\n\\\\n\"(0.009) \"]\\\\n\"(0.007) }\\\\n\\\\n(0.004)\\nwait \".\\\\n\\\\n\" (0.643) \" \"(0.206) \"?\\\\n\\\\n\"(0.053) \").\\\\n\\\\n\"(0.032) \"\\\\n\\\\n\"(0.021)\\n\" \\\\n\\\\n\"(0.015) \")\\\\n\\\\n\"(0.013) \"]\\\\n\\\\n\"(0.004) \":\\\\n\\\\n\"(0.003) \"?)\\\\n\\\\n\"(0.001)\\nA.5 Statistics of Different Size model\\n1.5B 7B 32B\\n80\\n85\\n90\\n95Accuracy (%)\\n83.20\\n92.80 92.80\\n1.5B 7B 32B\\n4000\\n5000\\n6000Avg T oken Num\\n5439\\n3975 3802\\n1.5B 7B 32B\\n0\\n5000\\n10000\\n15000Average Number of T okens\\n#=416\\n#=84\\n#=464\\n#=36\\n#=464\\n#=36\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n4000\\n6000\\n8000\\n10000T otal Wait Count\\n7936\\n10019\\n6896\\n4583\\n5680\\n6627\\nCorrect Wait Count Wrong Wait Count\\nFigure 9: Accuracy and output statistics of three models on the MATH500 dataset.\\n1.5B 7B 32B\\n30\\n40\\n50\\n60\\n70Accuracy (%)33.84\\n45.45\\n61.62\\n1.5B 7B 32B\\n5000\\n6000\\n7000\\n8000Avg T oken Num\\n7921\\n6111\\n5406\\n1.5B 7B 32B\\n0\\n2000\\n4000\\n6000\\n8000\\n10000Average Number of T okens\\n#=67\\n#=131\\n#=90\\n#=108\\n#=122\\n#=76\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n4000\\n6000\\n8000'),\n",
       " Document(metadata={}, page_content='7921\\n6111\\n5406\\n1.5B 7B 32B\\n0\\n2000\\n4000\\n6000\\n8000\\n10000Average Number of T okens\\n#=67\\n#=131\\n#=90\\n#=108\\n#=122\\n#=76\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n4000\\n6000\\n8000\\n10000T otal Wait Count\\n4878\\n8990\\n3087\\n5792\\n3881\\n4591\\nCorrect Wait Count Wrong Wait Count\\nFigure 10: Accuracy and output statistics of three models on the GPQA dataset.\\n16Preprint. Under review.\\n1.5B 7B 32B\\n70\\n80\\n90\\n100Accuracy (%)\\n75.00\\n92.50 95.00\\n1.5B 7B 32B\\n6000\\n8000\\n10000\\n12000Avg T oken Num\\n10460\\n6093\\n7106\\n1.5B 7B 32B\\n0\\n10000\\n20000\\n30000Average Number of T okens\\n#=30\\n#=10\\n#=37\\n#=3\\n#=38\\n#=2\\nCorrect Avg T okens Incorrect Avg T okens\\n1.5B 7B 32B\\n0\\n500\\n1000\\n1500\\n2000T otal Wait Count\\n1088\\n1470\\n887\\n611\\n974\\n772\\nCorrect Wait Count Wrong Wait Count\\nFigure 11: Accuracy and output statistics of three models on the AMC23 dataset.\\nA.6 Results of Non-reasoning model\\nTable 7: Accuracy, average output length, and estimated speed on four datasets. 1B-Instruct'),\n",
       " Document(metadata={}, page_content='A.6 Results of Non-reasoning model\\nTable 7: Accuracy, average output length, and estimated speed on four datasets. 1B-Instruct\\nrefers to Qwen-2.5-1.5B. “+” means with the help of reasoning models. Modify ratio indicates\\nthe proportion of tokens in the final output that come from target model. After applying\\nSpeculative Thinking, 1B-Instruct models demonstrate improvements in accuracy\\ndataset speculative target avg modify estimated acc\\npass@1 model model length ratio speed (%) Improv.\\nAIME 1B-Instruct\\nnormal 1701.5 – 224.4 4.4 –\\n+7B 14240.7 37.0% 76.9 8.9 +102.3%\\n+32B 15536.7 34.0% 51.6 10.0 +127.3%\\nGPQA 1B-Instruct\\nnormal 694.9 – 164.9 23.7 –\\n+7B 9019.3 26.0% 95.4 30.3 +27.8%\\n+32B 10500.2 26.0% 62.4 33.3 +40.5%\\nMATH500 1B-Instruct\\nnormal 1424.1 – 205.4 50.2 –\\n+7B 7947.2 30.0% 58.7 48.8 -2.9%\\n+32B 8935.7 29.0% 89.7 48.2 -4.0%\\nAMC23 1B-Instruct\\nnormal 1605.0 – 217.6 20.0 –\\n+7B 19376.5 23.0% 89.2 27.5 +37.5%\\n+32B 17114.4 23.0% 65.4 30.0 +50.0%\\n17')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5590a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string1 = \"\"\"[\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Who are the authors of the paper 'Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time'?\",\n",
    "      \"Which institutions are the authors affiliated with?\",\n",
    "      \"What key limitation of existing post-training approaches does this paper aim to overcome?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han\",\n",
    "      \"Case Western Reserve University and Carnegie Mellon University\",\n",
    "      \"They require costly training pipelines and still produce inefficient, overly lengthy outputs.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"By how much did the 1.5B model’s accuracy improve on MATH500 when assisted by the 32B reasoning model?\",\n",
    "      \"What was the reduction in average output length for the 1.5B model on MATH500 with assistance?\",\n",
    "      \"What accuracy improvement did the framework provide for Qwen-2.5-7B-Instruct on MATH500?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"6.2%, from 83.2% to 89.4%\",\n",
    "      \"15.7%, from 5439 tokens to 4583 tokens\",\n",
    "      \"7.8%, from 74.0% to 81.8%\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Which four datasets were used to evaluate speculative thinking in Figure 1?\",\n",
    "      \"What does '1.5B+32B' represent in the evaluation?\",\n",
    "      \"Where is the official code for speculative thinking available?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"AIME 2020–2024, MATH500, GPQA, and AMC23\",\n",
    "      \"The 32B model supervises reflective reasoning steps of the 1.5B model during inference.\",\n",
    "      \"https://github.com/uservan/speculative_thinking\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Why are smaller language models widely used in real-world applications?\",\n",
    "      \"What challenges do smaller models face on reasoning tasks?\",\n",
    "      \"What kind of post-training is often applied to improve smaller models?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Because of their lower computational and memory requirements.\",\n",
    "      \"They often underperform on tasks requiring complex reasoning.\",\n",
    "      \"Supervised fine-tuning on reasoning traces or reinforcement learning with verifiable signals.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What motivates the question of whether small reasoning models can be improved during inference without additional training?\",\n",
    "      \"What is speculative decoding?\",\n",
    "      \"Why are larger models impractical for many deployment scenarios?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Small models show limited improvements with training-free inference methods, while larger models are much stronger but costly.\",\n",
    "      \"An approach where a small model proposes tokens and a larger model verifies them to accelerate generation.\",\n",
    "      \"Because their inference cost and latency are too high.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What is the main difference between speculative thinking and speculative decoding?\",\n",
    "      \"Which structural cues are used to identify challenging reasoning segments?\",\n",
    "      \"How do larger models compare to smaller ones in handling reflective reasoning segments?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Speculative thinking operates at the reasoning level, while speculative decoding works at the token level.\",\n",
    "      \"Paragraph breaks followed by reflective phrases like 'wait' or 'alternatively'.\",\n",
    "      \"Larger models are more concise and effective at backtracking.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What accuracy improvements did a 1.5B model achieve with speculative thinking on AIME, MATH500, GPQA, and AMC23?\",\n",
    "      \"How did speculative thinking affect non-reasoning models like Qwen-2.5-7B-Instruct?\",\n",
    "      \"What is the overall paradigm shift that speculative thinking introduces?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"+6.6% on AIME, +6.2% on MATH500, +8.1% on GPQA, and +5.0% on AMC23\",\n",
    "      \"It gained +7.8% on MATH500 and +14.2% on GPQA with large model assistance.\",\n",
    "      \"A new inference-time paradigm fusing small-model efficiency with large-model reasoning strength.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What role does '\\\\n\\\\n' play in model reasoning processes?\",\n",
    "      \"Which reasoning-supportive tokens often appear after '\\\\n\\\\n'?\",\n",
    "      \"What dataset was used to analyze the distribution of preceding tokens for reasoning-supportive words?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"It acts as a structural clue, often triggering reflective or continuation behavior.\",\n",
    "      \"Tokens such as 'wait', 'hmm', and 'alternatively'.\",\n",
    "      \"MATH500 dataset.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"According to Table 1, which symbol most frequently precedes reasoning-supportive tokens?\",\n",
    "      \"What proportion of 'wait' tokens appear after '\\\\n\\\\n'?\",\n",
    "      \"What does this suggest about the role of '\\\\n\\\\n'?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"The newline symbol '\\\\n\\\\n'.\",\n",
    "      \"Over 80%.\",\n",
    "      \"It acts as a thinking cue prompting reflection or continuation.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What are the three segment types identified in case analysis using '\\\\n\\\\n'?\",\n",
    "      \"What does a reflection segment indicate?\",\n",
    "      \"What is suggested by the first sentence after each '\\\\n\\\\n'?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Affirmation, Reflection, and Statement.\",\n",
    "      \"That the model intends to reflect on its previous thought.\",\n",
    "      \"That it often contains reasoning-related cues.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Which models were compared in Section 2.2?\",\n",
    "      \"What dataset was used for their comparison?\",\n",
    "      \"What two performance metrics were analyzed?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Deepseek-distilled Qwen-2.5-32B, 7B, and 1.5B.\",\n",
    "      \"AIME 2022–2024 dataset.\",\n",
    "      \"Accuracy and output length.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What general trend was observed between model size, accuracy, and output length?\",\n",
    "      \"Why are incorrect responses typically longer than correct ones?\",\n",
    "      \"What role do reflective phrases play in incorrect responses of smaller models?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Larger models show higher accuracy and shorter outputs, while smaller models are less accurate and longer.\",\n",
    "      \"Because they contain excessive self-reflection and redundant reasoning.\",\n",
    "      \"They appear more frequently, signaling hesitation and ineffective backtracking.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What pattern do small models overuse when generating incorrect answers?\",\n",
    "      \"How does speculative thinking propose to use '\\\\n\\\\n'?\",\n",
    "      \"Why can larger models provide more accurate reasoning at reflective points?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"They overuse words like 'wait', indicating excessive self-reflection.\",\n",
    "      \"As a control point to delegate reasoning segments to larger models.\",\n",
    "      \"Because they are better at concise reasoning and avoiding redundant backtracking.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"In speculative thinking, what roles do the speculative and target models play?\",\n",
    "      \"What triggers an Affirmation/Reflection takeover?\",\n",
    "      \"What happens when the speculative model generates an affirmation or reflection sentence after a delimiter?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"The small model is the speculative model, and the large model is the target model providing supervision.\",\n",
    "      \"When a delimiter '\\\\n\\\\n' is followed by an affirmation or reflection sentence.\",\n",
    "      \"The target model takes over and generates the next n1 tokens.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What kind of takeover is triggered by verification-related cues?\",\n",
    "      \"What does the negativity counter 'c' track?\",\n",
    "      \"What auxiliary mechanism is used to prevent excessive reflection loops?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Verification takeover.\",\n",
    "      \"The number of reflection sentences generated after '\\\\n\\\\n'.\",\n",
    "      \"Inserting an auxiliary sentence and delegating the next n3 tokens to the target model.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What does the 'modify ratio' indicate in Table 2?\",\n",
    "      \"How did speculative thinking affect the 1.5B model’s estimated inference speed when paired with a 32B model on AIME?\",\n",
    "      \"What was the improvement in accuracy for the 1.5B model on GPQA with 32B assistance?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"The proportion of tokens in the final output that come from the target model.\",\n",
    "      \"+185.9% compared to the standalone 1.5B model.\",\n",
    "      \"+8.1% improvement.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Which benchmark datasets were used in the experiments described in Section 4?\",\n",
    "      \"What three evaluation metrics were used to assess speculative thinking?\",\n",
    "      \"How did speculative thinking affect the 1.5B model’s accuracy on AMC23?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"AIME 2022–2024, GPQA-Diamond, MATH500, and AMC23.\",\n",
    "      \"Accuracy, average output length, and estimated inference speed.\",\n",
    "      \"It increased by 5.0% with 32B assistance.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"How much output modification by the target model was needed to significantly improve speculative model reasoning?\",\n",
    "      \"How did the 1.5B speculative model assisted by 32B perform in terms of efficiency compared to the standalone 32B?\",\n",
    "      \"What trade-off does speculative thinking offer according to the analysis?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"About 20% of the speculative model’s output.\",\n",
    "      \"It outperformed the standalone 32B in generation speed.\",\n",
    "      \"A trade-off between performance and computational efficiency.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"In speculative decoding experiments, what was the size of the speculative and target models?\",\n",
    "      \"How many tokens at a time does Speculative Thinking take over during generation?\",\n",
    "      \"What major issue does speculative decoding face compared to Speculative Thinking?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"The speculative model was 7B and the target model was 32B.\",\n",
    "      \"20 tokens at a time.\",\n",
    "      \"It suffers from a high rejection rate where nearly 50% of tokens need to be regenerated by the target model.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Why does speculative thinking avoid the high rejection rate issue found in speculative decoding?\",\n",
    "      \"What are the two major categories of current approaches to enhancing LLM reasoning?\",\n",
    "      \"Which project achieved state-of-the-art reasoning performance using GRPO?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Because the target model only intervenes when necessary, rather than regenerating rejected tokens.\",\n",
    "      \"Reinforcement learning and supervised fine-tuning.\",\n",
    "      \"DeepSeek.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What was the goal of works replicating DeepSeek-R1?\",\n",
    "      \"Which studies emphasized the importance of reasoning step structure over content?\",\n",
    "      \"According to Ji et al. (2025), which part of reasoning instances is especially important for model performance?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"To uncover potential 'aha moments' in reasoning.\",\n",
    "      \"Li et al. (2025a).\",\n",
    "      \"The initial few tokens in each reasoning instance.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Which method was introduced by Kimi 1.5 to address verbose outputs in reasoning models?\",\n",
    "      \"What does TokenSkip aim to achieve?\",\n",
    "      \"What is the main idea behind LightThinker?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"The Long-to-Short method.\",\n",
    "      \"It improves efficiency by removing redundant or uninformative tokens from training data.\",\n",
    "      \"It compresses intermediate thoughts to produce shorter yet informative reasoning traces.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"According to Wang et al. (2025) and Sui et al. (2025a), what happens to model output length when reasoning fails?\",\n",
    "      \"Which method detects and terminates reasoning early?\",\n",
    "      \"What is the central proposal of Speculative Thinking in the conclusion section?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Outputs become significantly longer due to repetitive generation of supportive tokens like 'wait'.\",\n",
    "      \"Dynasor.\",\n",
    "      \"It leverages larger reasoning models to guide smaller ones through selective delegation at meaningful points.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What natural reasoning cues do LLMs exploit that Speculative Thinking leverages?\",\n",
    "      \"On which dataset did experiments show gains in accuracy, average output length, and efficiency?\",\n",
    "      \"What paradigm does the conclusion highlight for model collaboration?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"Reflection cues such as '\\\\n\\\\n'.\",\n",
    "      \"MATH500.\",\n",
    "      \"Collaborative inference between models of different capacities without additional training.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"What is one limitation of Speculative Thinking regarding the target model?\",\n",
    "      \"Why does the implementation assume both models are from the same family?\",\n",
    "      \"What example of a prompt format is considered important for achieving the best results?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"It requires the target model to have stronger reasoning abilities than the speculative model.\",\n",
    "      \"To leverage shared KV cache structures for faster inference.\",\n",
    "      \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\n",
    "    ]\n",
    "},\n",
    "  {\n",
    "    \"question\": [\n",
    "      \"Which keywords indicate reflection or hesitation in the framework?\",\n",
    "      \"If both reflection and affirmation cues appear in a sentence, which label is chosen in case of a tie?\",\n",
    "      \"What is the configured takeover length for verification-based intervention?\"\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "      \"“wait”, “alternatively”, “hold on”, “another”, “verify”, “think again”, “recap”, “check”.\",\n",
    "      \"Reflection.\",\n",
    "      \"125 tokens.\"\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5def7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(json_string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d344d062",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     16\u001b[39m     out = main_chain.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: example[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: example[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: out[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontexts\u001b[39m\u001b[33m\"\u001b[39m: out[\u001b[33m\"\u001b[39m\u001b[33mcontexts\u001b[39m\u001b[33m\"\u001b[39m],   \u001b[38;5;66;03m# correct key, list[str]\u001b[39;00m\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m\"\u001b[39m: example[\u001b[33m\"\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m predictions = [\u001b[43mrun_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m dataset]\n\u001b[32m     26\u001b[39m eval_dataset = Dataset.from_list(predictions)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 3. Evaluate with Ragas\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mrun_chain\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_chain\u001b[39m(example):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     out = \u001b[43mmain_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: example[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: out[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontexts\u001b[39m\u001b[33m\"\u001b[39m: out[\u001b[33m\"\u001b[39m\u001b[33mcontexts\u001b[39m\u001b[33m\"\u001b[39m],   \u001b[38;5;66;03m# correct key, list[str]\u001b[39;00m\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m\"\u001b[39m: example[\u001b[33m\"\u001b[39m\u001b[33mground_truth\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4781\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4767\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4768\u001b[39m \n\u001b[32m   4769\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4778\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4783\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4785\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4786\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4787\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4788\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1938\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1936\u001b[39m         output = cast(\n\u001b[32m   1937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1946\u001b[39m         )\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1948\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4639\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4637\u001b[39m                 output = chunk\n\u001b[32m   4638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4639\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4640\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4642\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mwith_docs\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_docs\u001b[39m(inputs):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# first get retrieved docs (not formatted, for eval)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     raw_docs = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     formatted_context = format_docs(raw_docs)\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# run model on {context, question}\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:256\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m task_type = \u001b[38;5;28mself\u001b[39m.task_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:207\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    205\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    206\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    209\u001b[39m         titles_batch = titles[\n\u001b[32m    210\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    211\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:131\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    124\u001b[39m current_text = texts[text_index]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    130\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    132\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:109\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    107\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "# install\n",
    "# pip install ragas langchain-openai datasets\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# suppose you already have your rag_chain (langchain Runnable)\n",
    "\n",
    "# 1. Prepare eval dataset (questions + ground truth answers)\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 2. Run your RAG chain and collect answers + retrieved contexts\n",
    "def run_chain(example):\n",
    "    out = main_chain.invoke({\"question\": example[\"question\"]})\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"answer\": out[\"answer\"],\n",
    "        \"contexts\": out[\"contexts\"],   # correct key, list[str]\n",
    "        \"ground_truth\": example[\"ground_truth\"]\n",
    "    }\n",
    "\n",
    "predictions = [run_chain(ex) for ex in dataset]\n",
    "\n",
    "eval_dataset = Dataset.from_list(predictions)\n",
    "\n",
    "# 3. Evaluate with Ragas\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67861e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out = \u001b[43mmain_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(out)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(out[\u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4781\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4767\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4768\u001b[39m \n\u001b[32m   4769\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4778\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4783\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4785\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4786\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4787\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4788\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1938\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1936\u001b[39m         output = cast(\n\u001b[32m   1937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1946\u001b[39m         )\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1948\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4639\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4637\u001b[39m                 output = chunk\n\u001b[32m   4638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4639\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4640\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4642\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mwith_docs\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_docs\u001b[39m(inputs):\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# get raw documents\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     raw_docs = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# 🔑 turn list[Document] into one string\u001b[39;00m\n\u001b[32m     23\u001b[39m     formatted_context = format_docs(raw_docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:256\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m task_type = \u001b[38;5;28mself\u001b[39m.task_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:207\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    205\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    206\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    209\u001b[39m         titles_batch = titles[\n\u001b[32m    210\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    211\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:131\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    124\u001b[39m current_text = texts[text_index]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    130\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    132\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:109\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    107\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "out = main_chain.invoke({\"question\": dataset[0][\"question\"]})\n",
    "print(out)\n",
    "print(type(out[\"answer\"]))\n",
    "print(type(out[\"contexts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af0bfe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m test_in = {\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: format_docs(\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: dataset[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m }\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(test_in[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(test_in[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:256\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m task_type = \u001b[38;5;28mself\u001b[39m.task_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:207\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    205\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    206\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    209\u001b[39m         titles_batch = titles[\n\u001b[32m    210\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    211\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:131\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    124\u001b[39m current_text = texts[text_index]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    130\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    132\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:109\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    107\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "test_in = {\n",
    "    \"context\": format_docs(retriever.invoke(dataset[0][\"question\"])),\n",
    "    \"question\": dataset[0][\"question\"]\n",
    "}\n",
    "print(type(test_in[\"context\"]))\n",
    "print(type(test_in[\"question\"]))\n",
    "\n",
    "print(\"CONTEXT PREVIEW:\", test_in[\"context\"][:200])\n",
    "print(\"QUESTION:\", test_in[\"question\"])\n",
    "\n",
    "# Now just run prompt directly\n",
    "print(prompt.format(**test_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "148b00a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTYPE of retriever output:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(docs))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst element:\u001b[39m\u001b[33m\"\u001b[39m, docs[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:256\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m task_type = \u001b[38;5;28mself\u001b[39m.task_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:207\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    205\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    206\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    209\u001b[39m         titles_batch = titles[\n\u001b[32m    210\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    211\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:131\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    124\u001b[39m current_text = texts[text_index]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    130\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    132\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:109\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    107\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(dataset[0][\"question\"])\n",
    "print(\"TYPE of retriever output:\", type(docs))\n",
    "print(\"First element:\", docs[0])\n",
    "print(\"Is list?\", isinstance(docs, list))\n",
    "\n",
    "formatted = format_docs(docs)\n",
    "print(\"TYPE of formatted:\", type(formatted))\n",
    "print(\"Preview formatted:\", formatted[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5a122c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84c3dc3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m formatted = format_docs(docs)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFORMATTED TYPE:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(formatted))       \u001b[38;5;66;03m# should be str\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:256\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m task_type = \u001b[38;5;28mself\u001b[39m.task_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:207\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    205\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    206\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    209\u001b[39m         titles_batch = titles[\n\u001b[32m    210\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    211\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:131\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    124\u001b[39m current_text = texts[text_index]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    130\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    132\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:109\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    107\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(dataset[0][\"question\"])\n",
    "formatted = format_docs(docs)\n",
    "\n",
    "print(\"FORMATTED TYPE:\", type(formatted))       # should be str\n",
    "print(\"QUESTION TYPE:\", type(dataset[0][\"question\"]))  # should be str\n",
    "\n",
    "print(prompt.input_variables)  # see what the prompt expects\n",
    "print(prompt.format(context=formatted, question=dataset[0][\"question\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9cc4cca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out = \u001b[43mwith_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mwith_docs\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_docs\u001b[39m(inputs):\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# raw retrieval\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     raw_docs = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# ✅ force convert list[Document] -> string\u001b[39;00m\n\u001b[32m     23\u001b[39m     formatted_context = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_docs])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:515\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    492\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    493\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    497\u001b[39m     **kwargs: Any,\n\u001b[32m    498\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    499\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.similarity_search_with_score_by_vector(\n\u001b[32m    517\u001b[39m         embedding,\n\u001b[32m    518\u001b[39m         k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m         **kwargs,\n\u001b[32m    522\u001b[39m     )\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:266\u001b[39m, in \u001b[36mFAISS._embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.embedding_function, Embeddings):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_function(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:256\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    253\u001b[39m \u001b[33;03m    Embedding for the text.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    255\u001b[39m task_type = \u001b[38;5;28mself\u001b[39m.task_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRETRIEVAL_QUERY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:207\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[39m\n\u001b[32m    205\u001b[39m embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    206\u001b[39m batch_start_index = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_prepare_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m titles:\n\u001b[32m    209\u001b[39m         titles_batch = titles[\n\u001b[32m    210\u001b[39m             batch_start_index : batch_start_index + \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    211\u001b[39m         ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:131\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._prepare_batches\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m    124\u001b[39m current_text = texts[text_index]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Number of tokens per a text is conservatively estimated\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# as 2 times number of words, punctuation and whitespace characters.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Using `count_tokens` API will make batching too expensive.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Utilizing a tokenizer, would add a dependency that would not\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# necessarily be reused by the application using this class.\u001b[39;00m\n\u001b[32m    130\u001b[39m current_text_token_cnt = (\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_split_by_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    132\u001b[39m     * \u001b[32m2\u001b[39m\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m end_of_batch = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_text_token_cnt > _MAX_TOKENS_PER_BATCH:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Current text is too big even for a single batch.\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# Such request will fail, but we still make a batch\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# so that the app can get the error from the API.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:109\u001b[39m, in \u001b[36mGoogleGenerativeAIEmbeddings._split_by_punctuation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    107\u001b[39m pattern = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m([\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_by\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Using re.split to split the text based on the pattern\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [segment \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m segment]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:267\u001b[39m, in \u001b[36msplit\u001b[39m\u001b[34m(pattern, string, maxsplit, flags, *args)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    262\u001b[39m     warnings.warn(\n\u001b[32m    263\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmaxsplit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is passed as positional argument\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m\n\u001b[32m    265\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "out = with_docs({\"question\": dataset[0][\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98e706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
